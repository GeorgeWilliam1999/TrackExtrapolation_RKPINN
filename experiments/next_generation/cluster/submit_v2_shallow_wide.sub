#===============================================================================
# HTCondor GPU Training - V2 Shallow & Wide Models
#
# Submits training jobs for shallow but wide networks based on timing analysis:
#   - DEPTH has less impact on timing than width (weak correlation r=0.37)
#   - WIDTH increases timing but enables more parallelism
#   - Shallow networks (1-2 layers) are faster due to less sequential computation
#
# V2 Strategy: 
#   - Use 1-2 hidden layers (shallow) instead of 2-4
#   - Use wider layers (256-1024 neurons) for more capacity
#   - This should maintain accuracy while improving inference speed
#
# Training: 20 epochs (longer than V1's 10 epochs)
#
# GPU Resources at NIKHEF:
#   - wn-lot-008/009: 2x Tesla V100-PCIE-32GB each
#   - wn-pijl-002-007: 2-4x NVIDIA L40S (45GB) each
#
# Usage:
#   condor_submit submit_v2_shallow_wide.sub
#
# Author: G. Scriven
# Date: January 2026
#===============================================================================

# Universe
universe = vanilla

# Executable
executable = run_training.sh

# GPU REQUIREMENTS
request_gpus = 1

# Job requirements - LONG time limit
+UseOS = "el9"
+JobCategory = "long"

# Resources (GPU training needs memory)
request_cpus = 4
request_memory = 32GB
request_disk = 10GB

# Maximum runtime: 12 hours (should be plenty for 20 epochs)
+MaxRuntime = 43200

# Working directory (NFS)
should_transfer_files = NO
initialdir = /data/bfys/gscriven/TE_stack/Rec/Tr/TrackExtrapolators/experiments/next_generation/cluster

# Logs
log = logs/v2_shallow_$(Cluster)_$(Process).log
output = logs/v2_shallow_$(Cluster)_$(Process).out
error = logs/v2_shallow_$(Cluster)_$(Process).err

# Notification
notification = Complete
notify_user = gscriven@nikhef.nl

#===============================================================================
# MLP V2 - SHALLOW & WIDE (8 variants)
#===============================================================================

# MLP V2-1: Ultra-shallow single layer (256 neurons)
# Architecture: 256 (~1.8k params) - minimal depth
arguments = mlp tiny mlp_v2_single_256 --epochs 20 --batch_size 8192 --hidden_dims 256
queue 1

# MLP V2-2: Ultra-shallow single layer (512 neurons)
# Architecture: 512 (~3.6k params) - wider single layer
arguments = mlp tiny mlp_v2_single_512 --epochs 20 --batch_size 8192 --hidden_dims 512
queue 1

# MLP V2-3: Ultra-shallow single layer (1024 neurons)
# Architecture: 1024 (~7.2k params) - maximum width single layer
arguments = mlp tiny mlp_v2_single_1024 --epochs 20 --batch_size 8192 --hidden_dims 1024
queue 1

# MLP V2-4: Shallow 2-layer (256-256)
# Architecture: 256-256 (~70k params) - balanced shallow-wide
arguments = mlp tiny mlp_v2_shallow_256 --epochs 20 --batch_size 8192 --hidden_dims 256 256
queue 1

# MLP V2-5: Shallow 2-layer (512-256)
# Architecture: 512-256 (~135k params) - asymmetric funnel
arguments = mlp tiny mlp_v2_shallow_512_256 --epochs 20 --batch_size 8192 --hidden_dims 512 256
queue 1

# MLP V2-6: Shallow 2-layer (512-512)
# Architecture: 512-512 (~268k params) - wide and shallow
arguments = mlp tiny mlp_v2_shallow_512 --epochs 20 --batch_size 8192 --hidden_dims 512 512
queue 1

# MLP V2-7: Shallow 2-layer (1024-256)
# Architecture: 1024-256 (~268k params) - very wide first layer
arguments = mlp tiny mlp_v2_shallow_1024_256 --epochs 20 --batch_size 8192 --hidden_dims 1024 256
queue 1

# MLP V2-8: Shallow 2-layer (1024-512)
# Architecture: 1024-512 (~530k params) - maximum shallow-wide
arguments = mlp tiny mlp_v2_shallow_1024_512 --epochs 20 --batch_size 8192 --hidden_dims 1024 512
queue 1

#===============================================================================
# PINN V2 - SHALLOW & WIDE (8 variants)
#===============================================================================

# PINN V2-1: Ultra-shallow single layer (256)
arguments = pinn tiny pinn_v2_single_256 --epochs 20 --batch_size 8192 --hidden_dims 256 --lambda_pde 1.0
queue 1

# PINN V2-2: Ultra-shallow single layer (512)
arguments = pinn tiny pinn_v2_single_512 --epochs 20 --batch_size 8192 --hidden_dims 512 --lambda_pde 1.0
queue 1

# PINN V2-3: Ultra-shallow single layer (1024)
arguments = pinn tiny pinn_v2_single_1024 --epochs 20 --batch_size 8192 --hidden_dims 1024 --lambda_pde 1.0
queue 1

# PINN V2-4: Shallow 2-layer (256-256)
arguments = pinn tiny pinn_v2_shallow_256 --epochs 20 --batch_size 8192 --hidden_dims 256 256 --lambda_pde 1.0
queue 1

# PINN V2-5: Shallow 2-layer (512-256)
arguments = pinn tiny pinn_v2_shallow_512_256 --epochs 20 --batch_size 8192 --hidden_dims 512 256 --lambda_pde 1.0
queue 1

# PINN V2-6: Shallow 2-layer (512-512)
arguments = pinn tiny pinn_v2_shallow_512 --epochs 20 --batch_size 8192 --hidden_dims 512 512 --lambda_pde 1.0
queue 1

# PINN V2-7: Shallow 2-layer (1024-256)
arguments = pinn tiny pinn_v2_shallow_1024_256 --epochs 20 --batch_size 8192 --hidden_dims 1024 256 --lambda_pde 1.0
queue 1

# PINN V2-8: Shallow 2-layer (1024-512)
arguments = pinn tiny pinn_v2_shallow_1024_512 --epochs 20 --batch_size 8192 --hidden_dims 1024 512 --lambda_pde 1.0
queue 1

#===============================================================================
# RK_PINN V2 - SHALLOW & WIDE (6 variants)
# Note: RK_PINN has inherent depth from 4-stage RK, so keep hidden networks shallow
#===============================================================================

# RK_PINN V2-1: Single hidden layer (256)
arguments = rk_pinn tiny rkpinn_v2_single_256 --epochs 20 --batch_size 4096 --hidden_dims 256 --lambda_pde 1.0
queue 1

# RK_PINN V2-2: Single hidden layer (512)
arguments = rk_pinn tiny rkpinn_v2_single_512 --epochs 20 --batch_size 4096 --hidden_dims 512 --lambda_pde 1.0
queue 1

# RK_PINN V2-3: Shallow 2-layer (256-256)
arguments = rk_pinn tiny rkpinn_v2_shallow_256 --epochs 20 --batch_size 4096 --hidden_dims 256 256 --lambda_pde 1.0
queue 1

# RK_PINN V2-4: Shallow 2-layer (512-256)
arguments = rk_pinn tiny rkpinn_v2_shallow_512_256 --epochs 20 --batch_size 4096 --hidden_dims 512 256 --lambda_pde 1.0
queue 1

# RK_PINN V2-5: Shallow 2-layer (512-512)
arguments = rk_pinn tiny rkpinn_v2_shallow_512 --epochs 20 --batch_size 4096 --hidden_dims 512 512 --lambda_pde 1.0
queue 1

# RK_PINN V2-6: Shallow 2-layer (1024-256)
arguments = rk_pinn tiny rkpinn_v2_shallow_1024_256 --epochs 20 --batch_size 4096 --hidden_dims 1024 256 --lambda_pde 1.0
queue 1

#===============================================================================
# SUMMARY: 22 V2 models total
#
# MLP:     8 models (single layer: 3, two layers: 5)
# PINN:    8 models (single layer: 3, two layers: 5)
# RK_PINN: 6 models (single layer: 2, two layers: 4)
#
# Expected timing improvements:
#   - Single layer models: ~0.8-1.5 μs/track (fastest)
#   - Two layer 256-256: ~1.1-1.5 μs/track
#   - Two layer 512-512: ~1.5-2.5 μs/track
#   - Two layer 1024-*: ~2-4 μs/track
#
# All should be faster than C++ RK4 reference (2.50 μs/track)
#===============================================================================

#!/usr/bin/env condor_submit
#
# HTCondor submission script for training data generation
#
# USES REAL FIELD MAP INTERPOLATION (twodip.rtf)
# This ensures consistency between training data and PINN physics loss.
#
# Fixed: 2026-01-19 - Now uses interpolated field map instead of approximation
#
# Usage:
#   condor_submit submit_condor.sub
#
# Monitor:
#   condor_q
#   condor_q -better-analyze <job_id>
#
# Author: G. Scriven
# Date: 2026-01-19 (FIXED: real field interpolation)
# ============================================================================

# Executable and arguments
# NOTE: --extrapolator is kept for backward compat but now ignored
#       Script now uses RK4 with REAL FIELD MAP (twodip.rtf)
executable              = /data/bfys/gscriven/conda/envs/TE/bin/python
arguments               = generate_cpp_data.py --n-tracks 10000 --output data/batch_$(Process).npz --seed $(Process) --z-start 4000 --z-end 12000 --p-min 0.5 --p-max 100 --step-size 5.0 --polarity -1

# Environment
environment             = "PATH=/data/bfys/gscriven/conda/envs/TE/bin:$ENV(PATH)"
getenv                  = True

# Job requirements - increased memory for field map loading
request_cpus            = 1
request_memory          = 4GB
request_disk            = 500MB
+UseOS                  = "el9"
+JobCategory            = "medium"

# File transfer - disabled because we're using NFS
should_transfer_files   = NO

# Working directory and logs
initialdir              = /data/bfys/gscriven/TE_stack/Rec/Tr/TrackExtrapolators/experiments/next_generation/data_generation
output                  = logs/job_$(Process).out
error                   = logs/job_$(Process).err
log                     = logs/condor_$(Cluster).log

# Job array: PRODUCTION RUN - 5000 jobs × 10k tracks = 50M tracks
# Using RK4 with REAL FIELD MAP INTERPOLATION (twodip.rtf)
# Interpolation error: O(h²) ~ 10⁻⁵ T (negligible)
# Estimated time: 10-30 min per job (field loading + RK4 integration)
# Output size: ~4 GB total (compressed)
queue 5000

# ============================================================================
# NOTES:
# ------
# 1. Uses RK4 integrator with REAL FIELD MAP (twodip.rtf)
#    - Trilinear interpolation of 81×81×146 grid
#    - Interpolation error: O(h²) ~ 10⁻⁵ T
#    - This matches the field used in PINN training!
#
# 2. Total dataset: 100 jobs × 10k tracks = 1,000,000 tracks
#    Storage: ~150-200 MB for full dataset (compressed npz format)
#
# 3. Each job is independent with different random seed = Process ID
#    This ensures diverse, non-overlapping training data
#
# 4. Output format: data/batch_000.npz, batch_001.npz, ..., batch_099.npz
#    Each file contains: X (inputs), Y (outputs), P (momenta), metadata
#
# 5. To merge all batches after completion:
#    python merge_batches.py --input "data/batch_*.npz" --output full_dataset.npz
#
# 6. Monitoring:
#    - condor_q: See all your jobs
#    - condor_q -hold: See held jobs
#    - condor_rm <cluster>: Remove all jobs in cluster
#    - tail -f logs/job_0.out: Watch first job progress
#
# 7. Scaling:
#    - For 10M tracks: Change queue to 1000
#    - For different extrapolator: Change --extrapolator argument
#      Options: BogackiShampine3, RungeKutta, Verner9, DormandPrince5
# ============================================================================

#!/usr/bin/env condor_submit
#
# HTCondor submission script for training data generation using C++ extrapolators
#
# This job array generates high-precision track extrapolation training data
# using the battle-tested LHCb C++ extrapolators (same as in benchmarking).
#
# Usage:
#   condor_submit submit_condor.sub
#
# Monitor:
#   condor_q
#   condor_q -better-analyze <job_id>
#
# Author: G. Scriven
# Date: 2025-01-14
# ============================================================================

# Executable and arguments
executable              = /data/bfys/gscriven/conda/envs/TE/bin/python
arguments               = generate_cpp_data.py --extrapolator RungeKutta --n-tracks 10000 --output data/batch_$(Process).npz --seed $(Process)

# Environment
environment             = "PATH=/data/bfys/gscriven/conda/envs/TE/bin:$ENV(PATH)"
getenv                  = True

# Job requirements
request_cpus            = 1
request_memory          = 2GB
request_disk            = 500MB
+UseOS                  = "el9"
+JobCategory            = "medium"

# File transfer - disabled because we're using NFS
should_transfer_files   = NO

# Working directory and logs
initialdir              = /data/bfys/gscriven/TE_stack/Rec/Tr/TrackExtrapolators/experiments/next_generation/data_generation
output                  = logs/job_$(Process).out
error                   = logs/job_$(Process).err
log                     = logs/condor_$(Cluster).log

# Job array: PRODUCTION RUN - 5000 jobs × 10k tracks = 50M tracks
# Using RungeKutta extrapolator (maximum precision, <0.01mm error)
# Estimated time: 3-6 hours (parallel execution on cluster)
# Output size: ~40 GB total (compressed)
queue 5000

# ============================================================================
# NOTES:
# ------
# 1. Uses C++ BogackiShampine3 extrapolator (fast & accurate, ~0.1mm error)
#    Alternative: RungeKutta for maximum precision (slower)
#
# 2. Total dataset: 100 jobs × 10k tracks = 1,000,000 tracks
#    Storage: ~150-200 MB for full dataset (compressed npz format)
#
# 3. Each job is independent with different random seed = Process ID
#    This ensures diverse, non-overlapping training data
#
# 4. Output format: data/batch_000.npz, batch_001.npz, ..., batch_099.npz
#    Each file contains: X (inputs), Y (outputs), P (momenta), metadata
#
# 5. To merge all batches after completion:
#    python merge_batches.py --input "data/batch_*.npz" --output full_dataset.npz
#
# 6. Monitoring:
#    - condor_q: See all your jobs
#    - condor_q -hold: See held jobs
#    - condor_rm <cluster>: Remove all jobs in cluster
#    - tail -f logs/job_0.out: Watch first job progress
#
# 7. Scaling:
#    - For 10M tracks: Change queue to 1000
#    - For different extrapolator: Change --extrapolator argument
#      Options: BogackiShampine3, RungeKutta, Verner9, DormandPrince5
# ============================================================================

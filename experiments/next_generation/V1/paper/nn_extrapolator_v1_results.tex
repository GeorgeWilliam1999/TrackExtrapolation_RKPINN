%===============================================================================
% Neural Network Track Extrapolator - V1 Analysis Report
%
% Author: G. Scriven
% Date: January 2026
% LHCb Track Extrapolation Project
%===============================================================================

\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{float}
\usepackage{subcaption}
\usepackage{listings}

% Simple macros to replace physics package
\newcommand{\vb}[1]{\mathbf{#1}}
\newcommand{\dd}[1]{\mathrm{d}#1}
\newcommand{\dv}[2]{\frac{\mathrm{d}#1}{\mathrm{d}#2}}
\newcommand{\pdv}[2]{\frac{\partial #1}{\partial #2}}

% No additional SI unit macros needed

\geometry{margin=2.5cm}

% Custom colors
\definecolor{mlpcolor}{RGB}{52, 152, 219}
\definecolor{pinncolor}{RGB}{155, 89, 182}
\definecolor{rkpinncolor}{RGB}{231, 76, 60}

% Title
\title{\textbf{Neural Network Track Extrapolator}\\
\Large V1/V2 Training Analysis Report}
\author{G. Scriven\\
LHCb Collaboration\\
NIKHEF, Amsterdam}
\date{January 2026}

\begin{document}

\maketitle

%===============================================================================
\begin{abstract}
We present the results of V1 and V2 training for neural network-based track extrapolation in the LHCb detector. Three architecture types were explored: Multi-Layer Perceptron (MLP), Physics-Informed Neural Network (PINN), and Runge-Kutta Physics-Informed Neural Network (RK\_PINN). A total of 75 model configurations were trained on 50 million track extrapolation samples. Our findings show that shallow-wide MLP architectures achieve inference times 3$\times$ faster than existing C++ extrapolators (0.83~$\mu$s vs 2.5~$\mu$s) while maintaining sub-0.1~mm position accuracy. Contrary to expectations, physics-informed approaches failed catastrophically due to a fundamental architecture flaw: the network learned to ignore the z-coordinate entirely. We identify the root cause and present PINN\_v3, a residual formulation that guarantees initial condition satisfaction by construction.
\end{abstract}

%===============================================================================
\section{Introduction}
\label{sec:intro}

Track extrapolation is a fundamental operation in particle physics reconstruction, required for pattern recognition, track fitting, and physics analysis. In LHCb, charged particles traverse the detector's magnetic field, following curved trajectories governed by the Lorentz force. The standard approach uses numerical integration (Runge-Kutta methods) of the equations of motion, which is accurate but computationally intensive.

This study investigates replacing the C++ Cash-Karp Runge-Kutta extrapolator with neural networks trained on simulated trajectory data. The potential benefits include:
\begin{itemize}
    \item \textbf{Speed:} Neural network inference can be parallelised on GPUs and optimised for specific hardware
    \item \textbf{Simplicity:} Single forward pass vs.\ iterative numerical integration
    \item \textbf{Differentiability:} Enables gradient-based optimisation of downstream tasks
\end{itemize}

The V1 training campaign explored three architecture families:
\begin{enumerate}
    \item \textbf{MLP:} Standard feedforward networks trained with supervised learning
    \item \textbf{PINN:} Networks augmented with physics-based loss terms
    \item \textbf{RK\_PINN:} Multi-stage architecture inspired by RK4 numerical integration
\end{enumerate}

%===============================================================================
\section{Mathematical Foundations}
\label{sec:maths}

\subsection{Track Extrapolation Problem}

A charged particle in a magnetic field follows a trajectory governed by the Lorentz force:
\begin{equation}
    \vb{F} = q(\vb{v} \times \vb{B})
\end{equation}
where $q$ is the particle charge, $\vb{v}$ is the velocity, and $\vb{B}$ is the magnetic field.

In LHCb, tracks are parameterised along the beam axis $z$. The state vector at position $z$ is:
\begin{equation}
    \vb{y}(z) = \begin{pmatrix} x \\ y \\ t_x \\ t_y \end{pmatrix}
\end{equation}
where $(x, y)$ is the transverse position and $(t_x, t_y) = (\dd{x}/\dd{z}, \dd{y}/\dd{z})$ are the track slopes.

The equations of motion in $z$-parameterisation are:
\begin{align}
    \dv{x}{z} &= t_x \\
    \dv{y}{z} &= t_y \\
    \dv{t_x}{z} &= \kappa N \left[ t_x t_y B_x - (1 + t_x^2) B_y + t_y B_z \right] \\
    \dv{t_y}{z} &= \kappa N \left[ (1 + t_y^2) B_x - t_x t_y B_y - t_x B_z \right]
\end{align}
where:
\begin{itemize}
    \item $\kappa = (q/p) \cdot c_\text{light}$ with $c_\text{light} = 2.99792458 \times 10^{-4}$ GeV/(T$\cdot$mm)
    \item $N = \sqrt{1 + t_x^2 + t_y^2}$ is the normalisation factor
    \item $q/p$ is the charge-over-momentum in 1/MeV
\end{itemize}

\subsection{Neural Network Formulation}

The extrapolation task is formulated as a regression problem:
\begin{equation}
    f_\theta : \mathbb{R}^6 \to \mathbb{R}^4
\end{equation}
\begin{equation}
    \vb{x} = (x_0, y_0, t_{x,0}, t_{y,0}, q/p, \Delta z) \mapsto \vb{y}_f = (x_f, y_f, t_{x,f}, t_{y,f})
\end{equation}
where $\vb{x}$ is the input (initial state + step size) and $\vb{y}_f$ is the predicted final state.

%===============================================================================
\section{Network Architectures}
\label{sec:architectures}

\subsection{MLP (Multi-Layer Perceptron)}

The MLP is a standard feedforward network (Figure~\ref{fig:mlp_arch}):
\begin{equation}
    \vb{y} = \sigma_{\text{out}} \circ W_L \circ \sigma \circ W_{L-1} \circ \cdots \circ \sigma \circ W_1 (\hat{\vb{x}})
\end{equation}
where $\hat{\vb{x}}$ is the normalised input, $W_i$ are linear transformations, and $\sigma$ is the SiLU activation:
\begin{equation}
    \sigma_{\text{SiLU}}(x) = x \cdot \text{sigmoid}(x) = \frac{x}{1 + e^{-x}}
\end{equation}

\textbf{Training loss:} Pure supervised learning
\begin{equation}
    \mathcal{L}_{\text{MLP}} = \frac{1}{N} \sum_{i=1}^{N} \| \hat{\vb{y}}_i - \vb{y}_i^* \|^2
\end{equation}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{../analysis/plots/mlp_architectures.png}
    \caption{MLP architecture configurations explored in V1 training, ranging from Tiny (5k parameters) to Large (400k parameters). Green: input layer (6 features). Blue: hidden layers. Red: output layer (4 state components).}
    \label{fig:mlp_arch}
\end{figure}

\subsection{PINN (Physics-Informed Neural Network)}

The PINN uses the same network structure but augments the loss with physics constraints (Figure~\ref{fig:pinn_flow}):
\begin{equation}
    \mathcal{L}_{\text{PINN}} = \mathcal{L}_{\text{data}} + \lambda_{\text{IC}} \mathcal{L}_{\text{IC}} + \lambda_{\text{PDE}} \mathcal{L}_{\text{PDE}}
\end{equation}

\textbf{Initial Condition Loss:} Ensures the network prediction at $z=0$ matches the input state:
\begin{equation}
    \mathcal{L}_{\text{IC}} = \| f_\theta(\vb{x}_0, z=0) - \vb{x}_0 \|^2
\end{equation}

\textbf{PDE Residual Loss:} Enforces the Lorentz force equations at collocation points $z_i$:
\begin{equation}
    \mathcal{L}_{\text{PDE}} = \sum_{i=1}^{N_c} \left\| \pdv{\hat{\vb{y}}_i}{z} - \vb{F}(\hat{\vb{y}}_i, \vb{B}(z_i)) \right\|^2
\end{equation}
where the derivatives are computed via automatic differentiation.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../analysis/plots/pinn_training_flow.png}
    \caption{PINN training flow showing the three loss components: data loss at endpoint, IC loss at $z=0$, and PDE residual at collocation points.}
    \label{fig:pinn_flow}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../analysis/plots/pinn_trajectory_loss.png}
    \caption{Visualisation of PINN loss components along a particle trajectory. Green: initial condition check. Purple: PDE residual at collocation points. Yellow: data loss at endpoint.}
    \label{fig:pinn_traj}
\end{figure}

\subsection{RK\_PINN (Runge-Kutta PINN)}

The RK\_PINN architecture is inspired by the classical RK4 integration scheme (Figure~\ref{fig:rkpinn_arch}):

\textbf{Classical RK4:}
\begin{align}
    k_1 &= f(t_n, y_n) \\
    k_2 &= f(t_n + h/2, y_n + h k_1/2) \\
    k_3 &= f(t_n + h/2, y_n + h k_2/2) \\
    k_4 &= f(t_n + h, y_n + h k_3) \\
    y_{n+1} &= y_n + \frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)
\end{align}

\textbf{RK\_PINN Network:}
\begin{itemize}
    \item Shared backbone extracts features from initial state
    \item Four stage heads predict state at $z = \{0.25, 0.5, 0.75, 1.0\} \cdot \Delta z$
    \item Learnable combination weights (initialised to RK4: $[1, 2, 2, 1]/6$)
\end{itemize}
\begin{equation}
    \hat{\vb{y}}_{\text{final}} = \sum_{k=1}^{4} w_k \cdot \hat{\vb{y}}_k, \quad w_k = \text{softmax}(\tilde{w}_k)
\end{equation}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../analysis/plots/rk_pinn_architecture.png}
    \caption{RK\_PINN multi-stage architecture with shared backbone and four stage heads at fractional z positions.}
    \label{fig:rkpinn_arch}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../analysis/plots/rk_pinn_trajectory_loss.png}
    \caption{RK\_PINN 4-stage predictions along a trajectory. Each stage contributes to the final prediction with learnable weights initialised to RK4 values.}
    \label{fig:rkpinn_traj}
\end{figure}

\subsection{Architecture Comparison}

Figure~\ref{fig:arch_comparison} summarises the key differences between the three architecture types.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../analysis/plots/architecture_comparison.png}
    \caption{Comparison of MLP, PINN, and RK\_PINN architectures showing network structure, loss functions, and characteristics.}
    \label{fig:arch_comparison}
\end{figure}

%===============================================================================
\section{Training Configuration}
\label{sec:training}

\subsection{Dataset}

Training data was generated using a C++ RK4 integrator with the LHCb magnetic field map:
\begin{itemize}
    \item \textbf{Samples:} 50 million track extrapolations
    \item \textbf{Split:} 80\% train, 10\% validation, 10\% test
    \item \textbf{Field model:} Interpolated from \texttt{twodip.rtf} field map
    \item \textbf{z range:} 0--9500 mm (full detector acceptance)
    \item \textbf{Momentum:} 1--100 GeV uniformly sampled
\end{itemize}

\subsection{Training Parameters}

\begin{table}[htbp]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Optimiser & AdamW \\
Learning rate & $10^{-3}$ \\
Weight decay & $10^{-4}$ \\
Batch size & 8192 \\
Epochs & 10 \\
Scheduler & Cosine annealing \\
Warmup epochs & 5 \\
Gradient clipping & 1.0 \\
\bottomrule
\end{tabular}
\caption{Training hyperparameters used for V1 models.}
\end{table}

\subsection{Model Configurations}

A total of 53 models were trained across the three architecture types:

\begin{table}[htbp]
\centering
\begin{tabular}{llccc}
\toprule
\textbf{Type} & \textbf{Hidden Dims} & \textbf{Params} & \textbf{Best Val Loss} & \textbf{Count} \\
\midrule
MLP & [64, 64] (tiny) & 5k & 0.0031 & 2 \\
MLP & [128, 128] (small) & 18k & 0.0013 & 3 \\
MLP & [256, 256, 128] (medium) & 100k & 0.0006 & 6 \\
MLP & [512, 512, 256] (large) & 399k & 0.0004 & 3 \\
MLP & [512, 512, 256, 128] (wide) & 431k & 0.0010 & 3 \\
\midrule
PINN & [256, 256, 128] (medium) & 100k & 0.0036 & 8 \\
PINN & [512, 512, 256] (large) & 399k & 0.0042 & 4 \\
\midrule
RK\_PINN & [256, 256, 128] (medium) & 202k & 0.0039 & 12 \\
RK\_PINN & [512, 512, 256] (large) & 797k & 0.0042 & 4 \\
\bottomrule
\end{tabular}
\caption{V1 model configurations and best validation loss achieved.}
\end{table}

%===============================================================================
\section{Results}
\label{sec:results}

\subsection{Accuracy Comparison}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{../analysis/plots/parameter_comparison.png}
    \caption{Parameter counts across architecture presets. Note that RK\_PINN has approximately 2$\times$ more parameters than MLP/PINN for the same hidden dimensions due to the 4 stage heads.}
    \label{fig:param_comparison}
\end{figure}

Table~\ref{tab:top_models} shows the best performing models from V1 training:

\begin{table}[htbp]
\centering
\begin{tabular}{lllcc}
\toprule
\textbf{Rank} & \textbf{Model} & \textbf{Type} & \textbf{Params} & \textbf{Val Loss} \\
\midrule
1 & mlp\_large\_v1 & MLP & 399k & 0.00044 \\
2 & mlp\_medium & MLP & 100k & 0.00058 \\
3 & rkpinn\_medium\_data\_only & RK\_PINN & 100k & 0.00067 \\
4 & mlp\_wide & MLP & 431k & 0.00095 \\
5 & mlp\_medium\_v1 & MLP & 100k & 0.00102 \\
6 & mlp\_wide\_v1 & MLP & 431k & 0.00117 \\
7 & rkpinn\_medium\_pde\_weak & RK\_PINN & 100k & 0.00131 \\
8 & mlp\_small & MLP & 18k & 0.00134 \\
9 & mlp\_balanced\_v1 & MLP & 57k & 0.00209 \\
10 & rkpinn\_wide & RK\_PINN & 431k & 0.00251 \\
\bottomrule
\end{tabular}
\caption{Top 10 V1 models ranked by validation loss.}
\label{tab:top_models}
\end{table}

\textbf{Key observation:} The top performing models are all MLPs or RK\_PINNs with physics loss disabled ($\lambda_{\text{PDE}} = 0$). Pure PINN models with physics constraints perform worse.

\subsection{Timing Performance}

Inference timing was benchmarked for all neural network models and compared against the existing C++ extrapolators in LHCb. The benchmarks were performed on CPU.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../analysis/plots/timing_benchmarks.png}
    \caption{Left: Neural network inference times by model. Right: Direct comparison between NN models and C++ extrapolators. Small MLPs achieve comparable timing to C++ implementations.}
    \label{fig:timing}
\end{figure}

\begin{table}[htbp]
\centering
\small
\begin{tabular}{llccc}
\toprule
\textbf{Type} & \textbf{Model} & \textbf{Params} & \textbf{Time ($\mu$s)} & \textbf{Notes} \\
\midrule
\multicolumn{5}{l}{\textit{C++ Extrapolators}} \\
C++ & Kisel & -- & 1.50 & Fast, 39.8 mm error \\
C++ & Herab & -- & 1.95 & 0.76 mm error \\
C++ & BogackiShampine3 & -- & 2.40 & 0.10 mm error \\
C++ & Reference (RK4) & -- & 2.50 & Ground truth \\
C++ & Verner9 & -- & 2.52 & High-order \\
\midrule
\multicolumn{5}{l}{\textit{Neural Network Models}} \\
MLP & mlp\_tiny & 4.9k & 1.12 & Fastest NN \\
PINN & pinn\_tiny & 4.9k & 1.19 & \\
MLP & mlp\_small & 17.9k & 1.22 & \\
PINN & pinn\_small & 17.9k & 1.31 & \\
MLP & mlp\_medium & 101k & 2.17 & Best accuracy/speed \\
PINN & pinn\_medium & 101k & 2.46 & \\
RK\_PINN & rkpinn\_tiny & 18.4k & 3.91 & RK overhead \\
RK\_PINN & rkpinn\_small & 69.5k & 4.06 & \\
PINN & pinn\_wide & 431k & 4.62 & \\
MLP & mlp\_wide & 431k & 4.70 & \\
RK\_PINN & rkpinn\_medium & 202k & 5.59 & \\
RK\_PINN & rkpinn\_wide & 532k & 7.83 & Slowest NN \\
\bottomrule
\end{tabular}
\caption{Timing comparison: C++ extrapolators vs neural network models (CPU inference).}
\label{tab:timing}
\end{table}

\textbf{Key findings:}
\begin{itemize}
    \item \textbf{Small MLPs are competitive:} mlp\_tiny (1.12~$\mu$s) and mlp\_small (1.22~$\mu$s) are faster than most C++ extrapolators
    \item \textbf{Medium models match C++:} mlp\_medium (2.17~$\mu$s) is comparable to the Reference RK4 (2.50~$\mu$s)
    \item \textbf{RK\_PINNs have overhead:} The multi-head architecture adds 2--4$\times$ overhead vs equivalent MLPs
    \item \textbf{Large models are slower:} Wide models (4.6--7.8~$\mu$s) are slower than C++ implementations
    \item \textbf{Trade-off:} mlp\_medium offers the best accuracy (val loss 0.0006) at comparable speed to C++
\end{itemize}

\subsection{Convergence Analysis}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../analysis/plots/loss_curves_architecture.png}
    \caption{Training and validation loss curves for different architectures.}
    \label{fig:loss_curves}
\end{figure}

%===============================================================================
\section{Error Analysis}
\label{sec:errors}

\subsection{Position Errors}

The position error (residual) is defined as:
\begin{equation}
    \varepsilon_{\text{pos}} = \sqrt{(x_{\text{pred}} - x_{\text{true}})^2 + (y_{\text{pred}} - y_{\text{true}})^2}
\end{equation}

For the best MLP model (mlp\_large\_v1):
\begin{itemize}
    \item Mean position error: 0.3 mm
    \item 95th percentile: 0.8 mm
    \item Maximum: 2.1 mm (outliers at very low momentum)
\end{itemize}

\subsection{Systematic Biases}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../analysis/plots/momentum_comparison.png}
    \caption{Error distribution as a function of particle momentum. Low momentum tracks show larger errors due to stronger bending.}
    \label{fig:momentum}
\end{figure}

Analysis revealed several systematic effects:

\begin{enumerate}
    \item \textbf{Momentum dependence:} Errors increase at low momentum ($p < 5$ GeV) where curvature is largest
    \item \textbf{Step size dependence:} Large extrapolation steps ($\Delta z > 5000$ mm) accumulate more error
    \item \textbf{Field region:} Errors peak in the dipole field region (4000--6000 mm)
\end{enumerate}

\subsection{PINN Training Failures}

Several PINN models exhibited training instabilities:

\begin{itemize}
    \item \textbf{NaN losses:} 15\% of PINN runs produced NaN gradients
    \item \textbf{Cause:} Physics loss scales as $\mathcal{O}(10^{26})$ when predictions are far from initialisation
    \item \textbf{Mitigation:} Reduced physics loss weight ($\lambda_{\text{PDE}} < 0.01$) or disabled entirely
\end{itemize}

\subsection{Why Physics-Informed Methods Underperformed}

Several hypotheses explain the poor performance of PINNs:

\begin{enumerate}
    \item \textbf{Data abundance:} With 50M samples, pure data-driven learning is sufficient; physics constraints become redundant
    \item \textbf{Field model mismatch:} PINN uses Gaussian field approximation; data generated with interpolated field map
    \item \textbf{Optimisation difficulty:} Multi-objective loss with competing gradients from data and physics terms
    \item \textbf{Computational overhead:} Automatic differentiation for physics loss is expensive
\end{enumerate}

%===============================================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{MLP is optimal:} Simple feedforward networks achieve the best accuracy/speed trade-off
    \item \textbf{Physics constraints hurt:} PINN and RK\_PINN are slower and less accurate than MLPs
    \item \textbf{Small models suffice:} Even the tiny MLP (5k params) achieves 1.12~$\mu$s inference, faster than most C++ extrapolators
    \item \textbf{Diminishing returns:} Beyond 100k parameters, accuracy improvements are marginal
\end{enumerate}

%===============================================================================
\section{Conclusion}
\label{sec:conclusion}

V1 and V2 training successfully demonstrated that neural networks can significantly outperform existing C++ extrapolators. The key findings are:

\begin{enumerate}
    \item \textbf{Shallow-wide MLPs are optimal:} A single-layer [256] MLP achieves 0.065~mm accuracy at 0.83~$\mu$s (3$\times$ faster than C++)
    \item \textbf{Physics-informed approaches failed:} PINN and RK\_PINN models exhibited 10,000$\times$ higher error than MLPs
    \item \textbf{Root cause identified:} The original PINN architecture allowed the network to ignore z\_frac entirely, learning a constant mapping without satisfying IC constraints
    \item \textbf{PINN\_v3 solution:} A residual formulation guarantees IC satisfaction by construction
\end{enumerate}

The best model from V2 (mlp\_v2\_shallow\_512\_256) achieves:
\begin{itemize}
    \item Position error: 0.028~mm (mean)
    \item Inference time: 1.93~$\mu$s (1.3$\times$ faster than C++ Reference)
\end{itemize}

For production deployment, we recommend \textbf{mlp\_v2\_single\_256} as the optimal balance:
\begin{itemize}
    \item Only 18k parameters (compact ONNX file)
    \item Inference time: 0.83~$\mu$s (3$\times$ faster than C++ Reference at 2.50~$\mu$s)
    \item Excellent accuracy (0.065~mm) for most use cases
\end{itemize}

V3 training will validate the PINN\_v3 architecture to determine whether physics-informed learning can compete with data-driven MLPs when the IC constraint is properly enforced.

%===============================================================================
\section{V2 Training Campaign and PINN Architecture Analysis}
\label{sec:v2}

Following V1, we conducted a V2 training campaign to investigate shallow-wide architectures and diagnose the persistent failures in physics-informed approaches.

\subsection{V2 Training Configuration}

Based on V1 findings that deep networks provided diminishing returns, V2 explored:
\begin{itemize}
    \item \textbf{Shallow architectures:} 1--2 hidden layers instead of 3--4
    \item \textbf{Wide layers:} 256, 512, 1024 neurons per layer
    \item \textbf{Extended training:} 50 epochs with 500k warmup steps
    \item \textbf{Models trained:} 22 configurations (8 MLP, 6 PINN, 8 RK\_PINN)
\end{itemize}

\subsection{V2 Results: State of the Art}

Table~\ref{tab:v2_top} shows the best performing models from V2 training:

\begin{table}[htbp]
\centering
\begin{tabular}{llcccc}
\toprule
\textbf{Model} & \textbf{Type} & \textbf{Layers} & \textbf{Error (mm)} & \textbf{Time ($\mu$s)} & \textbf{Speedup} \\
\midrule
mlp\_v2\_single\_256 & MLP & [256] & 0.065 & 0.83 & 3.0$\times$ \\
mlp\_v2\_shallow\_256 & MLP & [256, 256] & 0.033 & 1.28 & 2.0$\times$ \\
mlp\_v2\_shallow\_512\_256 & MLP & [512, 256] & 0.028 & 1.93 & 1.3$\times$ \\
\midrule
pinn\_v2\_shallow\_256 & PINN & [256, 256] & 593.4 & -- & -- \\
rkpinn\_v2\_shallow\_256 & RK\_PINN & [256, 256] & 2042.2 & -- & -- \\
\bottomrule
\end{tabular}
\caption{V2 top models. Speedup is relative to C++ Reference (2.5~$\mu$s). PINN and RK\_PINN models show catastrophic failures.}
\label{tab:v2_top}
\end{table}

\textbf{Key V2 Findings:}
\begin{enumerate}
    \item \textbf{Shallow-wide is optimal:} Single-layer [256] achieves 0.065 mm at 0.83~$\mu$s (3$\times$ speedup vs C++)
    \item \textbf{PINN/RK\_PINN failure confirmed:} Physics-informed models exhibit 10,000$\times$ higher error than MLPs
    \item \textbf{Diminishing returns confirmed:} [512, 256] achieves only 17\% better accuracy than [256, 256] at 50\% higher cost
\end{enumerate}

\subsection{PINN Failure Analysis}
\label{sec:pinn_failure}

Investigation of PINN/RK\_PINN training revealed a fundamental architectural flaw. During training, we observed (Figure~\ref{fig:pinn_failure}):

\begin{table}[htbp]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{MLP} & \textbf{PINN} \\
\midrule
Position error (mm) & 0.03 & 593.4 \\
IC error at $z=0$ (mm) & -- & 2758 \\
Physics loss trajectory & -- & Constant \\
Data loss trajectory & Decreasing & Decreasing \\
\bottomrule
\end{tabular}
\caption{PINN vs MLP training behaviour. IC error should be 0 by definition.}
\label{tab:pinn_failure}
\end{table}

\textbf{Root Cause:}

The PINN \texttt{forward()} method sets \texttt{x[:, 5] = 1.0} (z\_frac) before every forward pass:

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\small]
def forward(self, x):
    x = x.clone()
    x[:, 5] = 1.0  # Always evaluate at endpoint
    return self.forward_at_z(x)
\end{lstlisting}

\textbf{Consequence:} The network only ever sees \texttt{z\_frac = 1.0} during training. It learns a direct mapping from initial state to final state \textit{without learning the intermediate trajectory}. The physics loss at collocation points (which requires intermediate z values) evaluates a network that was never trained on those inputs.

\textbf{Evidence:}
\begin{enumerate}
    \item IC error of 2758~mm instead of 0~mm (network cannot satisfy IC)
    \item Physics loss remains constant while data loss decreases (network ignores physics)
    \item Network outputs constant prediction regardless of z\_frac input
\end{enumerate}

\subsection{PINN\_v3: Skip Connection Architecture}
\label{sec:pinn_v3}

To fix the fundamental IC constraint problem, we developed PINN\_v3 with a \textbf{residual formulation}:

\begin{equation}
    \vb{y}(z) = \vb{y}_0 + z \cdot \vb{\Delta}(\vb{y}_0, q/p)
\end{equation}

where $\vb{\Delta}$ is the network-predicted correction and $z$ is the normalised position (0 to 1).

\textbf{Key Properties:}
\begin{enumerate}
    \item \textbf{IC guaranteed:} At $z=0$, output equals initial state by construction
    \item \textbf{z\_frac modulation:} Corrections are \textit{multiplied} by z\_frac, so the network cannot ignore it
    \item \textbf{Residual learning:} Network learns corrections, not absolute positions (easier task)
\end{enumerate}

\textbf{Architecture:}
\begin{itemize}
    \item Input: $[x_0, y_0, t_{x,0}, t_{y,0}, q/p]$ (5 features, z\_frac excluded)
    \item Baseline: Straight-line extrapolation $\vb{y}_{\text{baseline}} = \vb{y}_0 + [t_x, t_y, 0, 0] \cdot z \cdot \Delta z$
    \item Network: Predicts corrections $[\delta t_x, \delta t_y, \delta x, \delta y]$
    \item Output: $\vb{y}_{\text{out}} = \vb{y}_{\text{baseline}} + z \cdot \vb{\delta}$
\end{itemize}

\textbf{Validation:}
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\small]
# Test PINN_v3 IC constraint
model = PINN_v3(...)
output_at_z0 = model.forward_at_z(x_input, z_frac=0.0)
ic_error = (output_at_z0 - initial_state).norm()
# Result: IC error = 0.000000 mm (perfect!)
\end{lstlisting}

Training infrastructure for PINN\_v3 has been prepared with configurations matching the successful V2 MLP models. V3 training will validate whether physics-informed learning can now compete with pure data-driven MLPs.

%===============================================================================
\appendix

\section{V1 Model Summary}
\label{app:models}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{../analysis/plots/v1_models_table.png}
    \caption{Complete list of V1 trained models with configurations.}
    \label{fig:v1_table}
\end{figure}

%===============================================================================
\end{document}

# Neural Network Track Extrapolators - Next Generation

**Project Goal:** Systematically train and compare neural network architectures for LHCb track extrapolation, storing all training metrics for comprehensive analysis.

**Status:** âœ… V1 Training Complete (53 models), âœ… V2 Training Complete (22 shallow-wide models), ğŸ”§ V3 Variable dz In Development

**Reference Baseline:** C++ RK4 (CashKarp): **2.50 Î¼s/track** (measured via TrackExtrapolatorTesterSOA)

---

## Directory Structure

```
next_generation/
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ DEPENDENCY_GRAPH.md    # Project dependencies
â”œâ”€â”€ V1/                    # V1 experiments (deprecated)
â”‚   â”œâ”€â”€ analysis/          # Analysis scripts and notebooks
â”‚   â”œâ”€â”€ benchmarking/      # C++ benchmarks
â”‚   â”œâ”€â”€ cluster/           # HTCondor jobs
â”‚   â”œâ”€â”€ data_generation/   # Data generation scripts
â”‚   â”œâ”€â”€ models/            # Neural network code
â”‚   â”œâ”€â”€ notes/             # Documentation
â”‚   â”œâ”€â”€ paper/             # Paper drafts
â”‚   â”œâ”€â”€ results/           # CSV results
â”‚   â”œâ”€â”€ trained_models/    # Symlinks to models
â”‚   â”œâ”€â”€ training/          # Training job scripts
â”‚   â””â”€â”€ utils/             # Utility modules
â”œâ”€â”€ V2/                    # V2 experiments (shallow-wide)
â”‚   â”œâ”€â”€ analysis/          # V2-specific analysis
â”‚   â”œâ”€â”€ cluster/           # V2 HTCondor jobs
â”‚   â”œâ”€â”€ data_generation/   # Same data as V1
â”‚   â”œâ”€â”€ models/            # Includes residual PINN
â”‚   â”œâ”€â”€ results/           # V2 results
â”‚   â”œâ”€â”€ trained_models/    # Symlinks to V2 models
â”‚   â”œâ”€â”€ training/          # V2 training configs
â”‚   â””â”€â”€ utils/             # Utility modules
â”œâ”€â”€ V3/                    # V3 experiments (variable dz) - ACTIVE
â”‚   â”œâ”€â”€ cluster/           # V3 HTCondor jobs
â”‚   â”œâ”€â”€ data_generation/   # Variable dz data gen
â”‚   â””â”€â”€ training/          # V3 training configs
â”œâ”€â”€ deployment/            # Export models to C++
â”œâ”€â”€ trained_models/        # All trained model checkpoints
â”œâ”€â”€ archive/               # Historical experiments
â””â”€â”€ explore_field_map.ipynb # Field map visualization
```

---

## âš ï¸ VERSION HISTORY

| Version | Status | Key Change | Limitation |
|---------|--------|------------|------------|
| **V1** | Deprecated | Initial experiments (53 models) | PINN IC failure, fixed dz=8000mm |
| **V2** | Deprecated | PINN residual fix, shallow-wide (22 models) | Still fixed dz=8000mm |
| **V3** | **Active** | Variable dz training (500-12000mm) | In development |

See version-specific documentation:
- [V1/README.md](V1/README.md) - Original experiments (deprecated)
- [V2/README.md](V2/README.md) - PINN residual architecture (partially functional)
- [V3/README.md](V3/README.md) - Variable dz support (current development)

---

## ğŸ† Current State of the Art (January 2026)

### Best Models

| Model | Position Error | Timing | Speedup vs C++ | Recommendation |
|-------|---------------|--------|----------------|----------------|
| `mlp_v2_single_256` | 0.065 mm | 0.83 Î¼s | **3.0Ã—** | â­ Best for speed |
| `mlp_v2_shallow_256` | 0.044 mm | 1.50 Î¼s | 1.7Ã— | Balanced |
| `mlp_v2_shallow_512_256` | **0.028 mm** | 1.93 Î¼s | 1.3Ã— | â­ Best accuracy |

### Key Findings

1. **MLP outperforms PINN/RK_PINN** - Simple MLPs achieve 0.03-0.07 mm accuracy, 10-100Ã— better than physics-informed models in V1/V2.

2. **Shallow-wide beats deep-narrow** - 1-2 layer networks with 256-1024 neurons outperform deeper architectures for both accuracy AND speed.

3. **10 models faster than C++ RK4** - The fastest (mlp_v2_single_256) achieves 3Ã— speedup with acceptable accuracy.

### âš ï¸ PINN/RK_PINN Failure Analysis

**Problem Discovered:** The original PINN and RK_PINN architectures have a fundamental flaw - they fail to satisfy the Initial Condition (IC) constraint:

| z_frac | PINN Output | Expected |
|--------|-------------|----------|
| 0.0 (IC) | x=2768 mm | x=207 mm |
| 1.0 | x=2752 mm | x=1039 mm |

The physics loss stays constant (~1.7) throughout training while data loss decreases. This means the network learned to **ignore z_frac entirely** and outputs nearly constant values.

**Root Cause:** The PINN forward pass sets `x[:, 5] = 1.0` for all inputs during training. The network can minimize data_loss at z=1 without learning the trajectory - it just learns a direct mapping from initial state to final state, ignoring the z_frac input.

**Solution: PINN_v3 Architecture** - Uses explicit skip connections that make z_frac impossible to ignore:
```
Output = InitialState + z_frac Ã— NetworkCorrection
```
At z=0, output = initial state (IC automatically satisfied!)

See [PINN_v3 Training](#pinn_v3-training) for details.

---

## Table of Contents

1. [Overview](#overview)
2. [Current State of the Art](#-current-state-of-the-art-january-2026)
3. [Experiment Design](#experiment-design)
4. [Model Architectures](#model-architectures)
5. [PINN_v3 Training](#pinn_v3-training)
6. [Training Configurations](#training-configurations)
7. [Running Experiments](#running-experiments)
8. [Loss Tracking & Convergence Analysis](#loss-tracking--convergence-analysis)
9. [Performance Benchmarking](#performance-benchmarking)
10. [Directory Structure](#directory-structure)
11. [Quick Reference](#quick-reference)

---

## Overview

We implement a systematic comparison of **three model architectures** with **varying complexity** and **physics constraints**:

| Architecture | Physics | Training Type | Key Characteristic |
|--------------|---------|---------------|-------------------|
| **MLP** | Implicit (from data) | Data loss only | Fast, simple baseline |
| **PINN** | Explicit (autodiff) | Data + PDE residual | Physics-constrained |
| **RK_PINN** | Explicit (multi-stage) | Data + staged PDE | RK4-inspired structure |

For each architecture, we test **four size presets**:

| Preset | Hidden Layers | Parameters | Use Case |
|--------|---------------|------------|----------|
| `tiny` | [64, 64] | ~5k | Debugging, quick tests |
| `small` | [128, 128] | ~20k | Fast training, baseline |
| `medium` | [256, 256, 128] | ~100k | Balanced performance |
| `wide` | [512, 512, 256, 128] | ~500k | Maximum accuracy |

This gives us **12 base experiments** (3 architectures Ã— 4 sizes), plus physics loss ablations.

---

## Experiment Design

### Core Experiments Matrix

We will train and compare all combinations:

| Model | tiny | small | medium | wide |
|-------|------|-------|--------|------|
| **MLP** | mlp_tiny | mlp_small | mlp_medium | mlp_wide |
| **PINN** | pinn_tiny | pinn_small | pinn_medium | pinn_wide |
| **RK_PINN** | rkpinn_tiny | rkpinn_small | rkpinn_medium | rkpinn_wide |

### Physics Loss Ablations (PINN/RK_PINN)

For PINN and RK_PINN models, we study the effect of physics loss weights:

| Experiment | Î»_data | Î»_pde | Î»_ic | Purpose |
|------------|--------|-------|------|---------|
| `data_only` | 1.0 | 0.0 | 0.0 | Baseline (equivalent to MLP) |
| `pde_weak` | 1.0 | 0.01 | 0.01 | Weak physics regularization |
| `pde_balanced` | 1.0 | 1.0 | 1.0 | Equal weighting (default) |
| `pde_strong` | 1.0 | 10.0 | 10.0 | Strong physics enforcement |
| `pde_dominant` | 0.1 | 1.0 | 1.0 | Physics-dominated training |

### Momentum Range Studies

Physics behavior varies with momentum (low-p tracks bend more):

| Study | Momentum Range | Expected Behavior |
|-------|----------------|-------------------|
| `low_p` | 0.5 - 5 GeV | High curvature, physics crucial |
| `mid_p` | 5 - 20 GeV | Moderate curvature |
| `high_p` | 20 - 100 GeV | Low curvature, nearly linear |
| `full_range` | 0.5 - 100 GeV | Full domain (production) |

---

## Model Architectures

### 1. MLP (Multi-Layer Perceptron)

**Philosophy:** Learn the inputâ†’output mapping directly from data.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MLP Architecture                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   Input [6]              Hidden Layers              Output [4]  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ xâ‚€      â”‚           â”‚             â”‚            â”‚ x_f     â”‚  â”‚
â”‚  â”‚ yâ‚€      â”‚    â”€â”€â”€â–º   â”‚  FC + ReLU  â”‚   â”€â”€â”€â–º     â”‚ y_f     â”‚  â”‚
â”‚  â”‚ txâ‚€     â”‚           â”‚  FC + ReLU  â”‚            â”‚ tx_f    â”‚  â”‚
â”‚  â”‚ tyâ‚€     â”‚           â”‚  ...        â”‚            â”‚ ty_f    â”‚  â”‚
â”‚  â”‚ q/p     â”‚           â”‚             â”‚            â”‚         â”‚  â”‚
â”‚  â”‚ dz      â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Loss:** `data_loss = MSE(prediction, ground_truth)`

**Advantages:** Fastest inference, simplest to train  
**Disadvantages:** No physics constraints, may extrapolate poorly

---

### 2. PINN (Physics-Informed Neural Network)

**Philosophy:** Constrain learning with Lorentz force equations via autodiff.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        PINN Architecture                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   Input [6]              Hidden Layers              Output [4]  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ xâ‚€      â”‚           â”‚             â”‚            â”‚ x(z)    â”‚  â”‚
â”‚  â”‚ yâ‚€      â”‚    â”€â”€â”€â–º   â”‚  FC + ReLU  â”‚   â”€â”€â”€â–º     â”‚ y(z)    â”‚  â”‚
â”‚  â”‚ txâ‚€     â”‚           â”‚  FC + ReLU  â”‚            â”‚ tx(z)   â”‚  â”‚
â”‚  â”‚ tyâ‚€     â”‚           â”‚  ...        â”‚            â”‚ ty(z)   â”‚  â”‚
â”‚  â”‚ q/p     â”‚           â”‚             â”‚            â”‚         â”‚  â”‚
â”‚  â”‚ z_norm  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚       â”‚
â”‚       â–²                                                â”‚       â”‚
â”‚       â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚       â”‚
â”‚       â”‚              â”‚  PHYSICS LOSS   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚       â”‚              â”‚  âˆ‚y/âˆ‚z = F(y,B) â”‚   (autodiff)          â”‚
â”‚       â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚       â”‚                      â”‚                                 â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â”‚            (collocation points: z âˆˆ [0,1])                     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Losses:**
- `data_loss = MSE(y(z=1), ground_truth)` â€” endpoint match
- `ic_loss = MSE(y(z=0), initial_state)` â€” initial condition
- `pde_loss = Î£ ||dy/dz - Lorentz(y, B)||Â²` â€” PDE residual

**Advantages:** Physics-constrained, better generalization  
**Disadvantages:** Slower training (autodiff overhead), requires field model

---

### 3. RK_PINN (Runge-Kutta PINN)

**Philosophy:** Multi-stage architecture inspired by RK4 numerical integrator.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RK_PINN Architecture                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   Input [6]           Backbone           Stage Heads            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ xâ‚€      â”‚        â”‚         â”‚   â”Œâ”€â”€â”€â–ºâ”‚ Head 1      â”‚â”€â”€â–º kâ‚   â”‚
â”‚  â”‚ yâ‚€      â”‚  â”€â”€â”€â–º  â”‚ Shared  â”‚   â”‚    â”‚ (z = 0.25)  â”‚         â”‚
â”‚  â”‚ txâ‚€     â”‚        â”‚ FC      â”‚   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚  â”‚ tyâ‚€     â”‚        â”‚ Layers  â”‚â”€â”€â”€â”¼â”€â”€â”€â–ºâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ q/p     â”‚        â”‚         â”‚   â”‚    â”‚ Head 2      â”‚â”€â”€â–º kâ‚‚   â”‚
â”‚  â”‚ dz      â”‚        â”‚         â”‚   â”‚    â”‚ (z = 0.5)   â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                   â”œâ”€â”€â”€â–ºâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚                                   â”‚    â”‚ Head 3      â”‚â”€â”€â–º kâ‚ƒ   â”‚
â”‚                                   â”‚    â”‚ (z = 0.75)  â”‚         â”‚
â”‚                                   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                   â””â”€â”€â”€â–ºâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚                                        â”‚ Head 4      â”‚â”€â”€â–º kâ‚„   â”‚
â”‚                                        â”‚ (z = 1.0)   â”‚         â”‚
â”‚                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                 â”‚
â”‚   Output = wâ‚Â·kâ‚ + wâ‚‚Â·kâ‚‚ + wâ‚ƒÂ·kâ‚ƒ + wâ‚„Â·kâ‚„                       â”‚
â”‚                                                                 â”‚
â”‚   Weights: learnable, initialized to [1,2,2,1]/6 (RK4)         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Losses:**
- `data_loss = MSE(weighted_sum, ground_truth)`
- `stage_pde_loss = Î£áµ¢ ||âˆ‚yáµ¢/âˆ‚z - Lorentz(yáµ¢, B(záµ¢))||Â²` (at each stage)

**Advantages:** Interpretable stages, natural multi-scale learning  
**Disadvantages:** More complex architecture, 4Ã— head computations

---

### 4. PINN_v3 (Physics-Informed with Skip Connections) â­ NEW

**Philosophy:** Fix the IC constraint problem with residual formulation.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PINN_v3 Architecture                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   Input [5]                                  Output [4]         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Encoder     Correction                          â”‚
â”‚  â”‚ xâ‚€      â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚  â”‚ yâ‚€      â”‚â”€â”€â–ºâ”‚ FC    â”‚â”€â”€â”€â–ºâ”‚ Head  â”‚â”€â”€â–º [Î”tx, Î”ty, Î”x, Î”y]    â”‚
â”‚  â”‚ txâ‚€     â”‚   â”‚ FC    â”‚    â””â”€â”€â”€â”¬â”€â”€â”€â”˜                          â”‚
â”‚  â”‚ tyâ‚€     â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚                              â”‚
â”‚  â”‚ q/p     â”‚                    â”‚ Correction                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â–¼                              â”‚
â”‚       â”‚                   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—    â”‚
â”‚       â”‚  z_frac â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•‘  Output = Initial +           â•‘    â”‚
â”‚       â”‚    â”‚              â•‘          z_frac Ã— Correction  â•‘    â”‚
â”‚       â”‚    â”‚              â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â”‚
â”‚       â”‚    â”‚                    â”‚                              â”‚
â”‚       â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                              â”‚
â”‚       â”‚                         â”‚                              â”‚
â”‚       â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚       â”‚   â”‚  x_out = xâ‚€ + txâ‚€Ã—zÃ—dz + zÃ—Î”x            â”‚        â”‚
â”‚       â”‚   â”‚  y_out = yâ‚€ + tyâ‚€Ã—zÃ—dz + zÃ—Î”y            â”‚        â”‚
â”‚       â”‚   â”‚  tx_out = txâ‚€ + zÃ—Î”tx                     â”‚        â”‚
â”‚       â”‚   â”‚  ty_out = tyâ‚€ + zÃ—Î”ty                     â”‚        â”‚
â”‚       â””â”€â”€â”€â–ºâ”‚                                          â”‚        â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                 â”‚
â”‚   KEY: At z=0, Output = Initial State (IC GUARANTEED!)         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Innovation:**
- **Residual formulation:** Output = InitialState + z_frac Ã— NetworkCorrection
- At z=0: Output exactly equals initial state (IC is **automatically satisfied**)
- At z=1: Network learns the full displacement
- **z_frac modulation:** Network CANNOT ignore z_frac because correction is multiplied by it

**Losses:**
- `data_loss = MSE(y(z=1), ground_truth)` â€” endpoint match
- `ic_loss = ~0` â€” automatically satisfied by construction!
- `pde_loss = Î£ ||dy/dz - Lorentz(y, B)||Â²` â€” PDE residual

**Advantages:**
- IC constraint satisfied by design (no optimization needed)
- Physics loss actually contributes to gradient
- Network learns corrections, not raw outputs (easier task)

**Disadvantages:** Slightly more complex architecture, assumes straight-line baseline

---

## PINN_v3 Training

### Why PINN_v3?

The original PINN/RK_PINN architectures failed because:
1. Network could minimize data_loss without learning physics
2. IC constraint not enforced in network structure
3. z_frac input was effectively ignored

PINN_v3 fixes this with a residual formulation that **guarantees** IC satisfaction.

### Training PINN_v3 Models

```bash
# Train single model
cd experiments/next_generation
python training/train_pinn_v3.py --preset pinn_v3_shallow_256

# Submit all V3 configurations to cluster
condor_submit cluster/submit_pinn_v3.sub

# Submit specific configuration
condor_submit cluster/submit_pinn_v3.sub -append "Queue experiment_name from (pinn_v3_shallow_512)"
```

### V3 Configurations

| Model | Hidden Dims | Expected Timing | Purpose |
|-------|-------------|-----------------|---------|
| `pinn_v3_single_256` | [256] | ~1 Î¼s | Fastest physics-informed |
| `pinn_v3_single_512` | [512] | ~1.5 Î¼s | Balance |
| `pinn_v3_shallow_256` | [256, 256] | ~2 Î¼s | Baseline |
| `pinn_v3_shallow_512` | [512, 512] | ~3 Î¼s | High capacity |
| `pinn_v3_shallow_512_256` | [512, 256] | ~2.5 Î¼s | Tapered |
| `pinn_v3_shallow_1024_256` | [1024, 256] | ~4 Î¼s | Maximum capacity |

### Expected Outcomes

With PINN_v3:
- IC error should be **exactly 0** (by construction)
- PDE loss should decrease during training (unlike original PINN)
- Final accuracy may be similar to MLP but with physics guarantees
- Useful for extrapolation beyond training domain

---

## Training Configurations

### Default Hyperparameters

```yaml
# Data
data_path: data_generation/data/full_dataset_50M.npz
train_fraction: 0.8    # 40M training samples
val_fraction: 0.1      # 5M validation samples
test_fraction: 0.1     # 5M test samples

# Training
batch_size: 2048       # Optimized for GPU memory
epochs: 100            # Maximum epochs
learning_rate: 1e-3    # Initial LR
weight_decay: 1e-4     # L2 regularization
scheduler: cosine      # LR schedule with warmup

# Early Stopping
patience: 20           # Stop if no improvement for 20 epochs
min_delta: 1e-6        # Minimum improvement threshold

# Physics (PINN/RK_PINN)
lambda_pde: 1.0        # PDE residual weight
lambda_ic: 1.0         # Initial condition weight
n_collocation: 10      # Collocation points (PINN only)
```

### Experiment-Specific Configs

Each experiment will have a YAML config file in `configs/`:

```yaml
# configs/mlp_medium.yaml
model_type: mlp
preset: medium
experiment_name: mlp_medium_v1

# configs/pinn_medium_strong_physics.yaml
model_type: pinn
preset: medium
lambda_pde: 10.0
lambda_ic: 10.0
experiment_name: pinn_medium_strong_v1
```

---

## Running Experiments

### Step 1: Generate Training Data (âœ… Complete)

Training data has been generated and merged:
- `data_generation/data/training_50M.npz` - 50M tracks (3.7GB)
- `data_generation/data/training_low_p.npz` - 10M tracks (p < 5 GeV)
- `data_generation/data/training_mid_p.npz` - 10M tracks (5 â‰¤ p < 20 GeV)
- `data_generation/data/training_high_p.npz` - 10M tracks (p â‰¥ 20 GeV)

```bash
# Verify data:
python -c "import numpy as np; d=np.load('data_generation/data/training_50M.npz'); print(f'Loaded {d[\"X\"].shape[0]:,} tracks')"
```

### Step 2: Run Single Experiment

```bash
cd models

# Train MLP (data loss only)
python train.py --model mlp --preset medium --epochs 100 --name mlp_medium_v1

# Train PINN (data + physics)
python train.py --model pinn --preset medium --lambda_pde 1.0 --epochs 100 --name pinn_medium_v1

# Train RK_PINN (data + staged physics)
python train.py --model rk_pinn --preset medium --lambda_pde 1.0 --epochs 100 --name rkpinn_medium_v1
```

### Step 3: Run Full Experiment Suite

```bash
# Run all experiments via unified runner
python run_all_experiments.py --list        # List available experiments
python run_all_experiments.py --all         # Submit all to HTCondor
python run_all_experiments.py --local       # Run locally (interactive)

# Or submit individual jobs
cd training/jobs
condor_submit mlp_medium.sub
```

### Step 4: Evaluate and Compare

```bash
cd models

# Evaluate single model
python evaluate.py --model_path ../trained_models/mlp_medium/best_model.pt

# Run comprehensive analysis
cd ../analysis
jupyter notebook experiment_analysis.ipynb
```

---

## Loss Tracking & Convergence Analysis

### What We Store

Every training run saves comprehensive loss information:

```
checkpoints/<experiment_name>/
â”œâ”€â”€ config.json              # Full training configuration
â”œâ”€â”€ best_model.pt            # Model with best validation loss
â”œâ”€â”€ normalization.json       # Input/output normalization parameters
â”œâ”€â”€ history.json             # Complete training history (see below)
â”œâ”€â”€ model_config.json        # Architecture configuration
â””â”€â”€ checkpoint_epoch_N.pt    # Periodic checkpoints
```

### History Format (history.json)

```json
{
  "train": [
    {
      "epoch": 1,
      "loss": 0.0234,
      "data_loss": 0.0230,
      "physics_loss": 0.0004,
      "ic_loss": 0.0002,
      "pde_loss": 0.0002,
      "lr": 0.001
    }
  ],
  "val": [
    {
      "epoch": 1,
      "loss": 0.0198,
      "pos_mean_mm": 0.342,
      "pos_std_mm": 0.156,
      "pos_95_mm": 0.621,
      "slope_mean": 1.2e-5,
      "x_mean_mm": 0.241,
      "y_mean_mm": 0.243,
      "tx_mean": 8.5e-6,
      "ty_mean": 8.7e-6
    }
  ],
  "test_final": { },
  "best_epoch": 87,
  "best_val_loss": 0.00156,
  "training_time": 3247.5
}
```

### Stored Loss Components

| Loss Component | Models | Description |
|----------------|--------|-------------|
| `loss` | All | Total combined loss |
| `data_loss` | All | MSE(prediction, ground_truth) |
| `physics_loss` | PINN, RK_PINN | ic_loss + pde_loss |
| `ic_loss` | PINN, RK_PINN | Initial condition constraint |
| `pde_loss` | PINN, RK_PINN | Lorentz equation residual |
| `stage_losses` | RK_PINN | Per-stage PDE losses [4 values] |

### Convergence Analysis

After training, analyze convergence behavior using the analysis notebook or scripts:

```bash
cd analysis

# Main analysis notebook (recommended)
jupyter notebook experiment_analysis.ipynb

# Or use command-line tools:
python analyze_models.py --checkpoint_dir ../trained_models
python timing_benchmark.py --models_dir ../trained_models
```

#### Key Convergence Questions

1. **Does physics loss help?** Compare final accuracy: MLP vs PINN vs RK_PINN
2. **How fast do losses converge?** Plot epochs to reach 90% of final accuracy
3. **Does physics loss converge?** Track ic_loss and pde_loss separately
4. **Model size vs convergence speed?** Compare tiny/small/medium/wide
5. **Overfitting detection?** Plot train vs validation loss divergence

---

## Performance Benchmarking

### Metrics We Compare

#### Accuracy Metrics

| Metric | Target | Description |
|--------|--------|-------------|
| `pos_mean_mm` | < 0.1 mm | Mean position error (âˆš(Î”xÂ² + Î”yÂ²)) |
| `pos_95_mm` | < 0.3 mm | 95th percentile position error |
| `slope_mean` | < 1e-5 | Mean slope error (âˆš(Î”txÂ² + Î”tyÂ²)) |
| `x_mean_mm` | < 0.1 mm | Mean x position error |
| `y_mean_mm` | < 0.1 mm | Mean y position error |

#### Speed Metrics

| Metric | Measurement Method |
|--------|-------------------|
| `training_time` | Total wall-clock time for training |
| `inference_time_cpu` | Time per batch on CPU |
| `inference_time_gpu` | Time per batch on GPU |
| `throughput_gpu` | Tracks/second on GPU |

### Benchmark Protocol

```bash
cd analysis

# Run timing benchmarks on all trained models
python timing_benchmark.py \
    --models_dir ../trained_models \
    --data_path ../data_generation/data/training_50M.npz \
    --output results/timing_results.json

# Generate paper-quality plots
python generate_paper_quality_plots.py
```

### Expected Results Table

After all experiments complete, we generate a summary table:

| Model | Params | pos_mean (mm) | pos_95% (mm) | Time (Î¼s/track) | vs C++ (2.50 Î¼s) |
|-------|--------|---------------|--------------|-----------------|------------------|
| mlp_tiny | 5,252 | 0.024 | 0.052 | 1.10 | 2.27Ã— faster |
| mlp_small | 20,228 | 0.023 | 0.051 | 1.15 | 2.17Ã— faster |
| mlp_medium | 99,588 | 0.022 | 0.049 | 1.35 | 1.85Ã— faster |
| mlp_wide | 467,972 | 0.021 | 0.047 | 1.75 | 1.43Ã— faster |
| pinn_weak | ~20k | 0.030 | 0.065 | 1.55 | 1.61Ã— faster |
| pinn_moderate | ~100k | 0.028 | 0.061 | 2.10 | 1.19Ã— faster |
| rkpinn_coll5 | ~25k | 0.025 | 0.055 | 3.10 | 0.81Ã— slower |
| rkpinn_coll10 | ~25k | 0.024 | 0.053 | 3.60 | 0.69Ã— slower |
| **C++ RK4** | N/A | 0.000 (ref) | 0.000 (ref) | **2.50** (baseline) | 1.00Ã— |

**Note:** All ML models achieve sub-0.1 mm position accuracy with sub-1e-5 slope error, meeting physics requirements.

---

## Directory Structure

```
next_generation/
â”œâ”€â”€ README.md                    # This file - experiment plan
â”œâ”€â”€ run_all_experiments.py       # â­ Unified experiment runner script
â”‚
â”œâ”€â”€ data_generation/             # Training data generation
â”‚   â”œâ”€â”€ README.md               # Field map and data format docs
â”‚   â”œâ”€â”€ generate_data.py        # Main data generator
â”‚   â”œâ”€â”€ generate_cpp_data.py    # C++ extrapolator data wrapper
â”‚   â”œâ”€â”€ merge_batches.py        # Combine HTCondor batch outputs
â”‚   â”œâ”€â”€ create_momentum_splits.py # Split by momentum range
â”‚   â””â”€â”€ data/                   # Generated datasets
â”‚       â”œâ”€â”€ training_50M.npz    # Full dataset (50M tracks)
â”‚       â”œâ”€â”€ training_low_p.npz  # Low momentum (0.5-5 GeV, 10M)
â”‚       â”œâ”€â”€ training_mid_p.npz  # Mid momentum (5-20 GeV, 10M)
â”‚       â””â”€â”€ training_high_p.npz # High momentum (20-100 GeV, 10M)
â”‚
â”œâ”€â”€ models/                      # Model definitions and training
â”‚   â”œâ”€â”€ README.md               # Detailed architecture documentation
â”‚   â”œâ”€â”€ architectures.py        # MLP, PINN, RK_PINN classes
â”‚   â”œâ”€â”€ train.py                # â­ Main training script
â”‚   â”œâ”€â”€ evaluate.py             # Model evaluation
â”‚   â”œâ”€â”€ export_onnx.py          # ONNX export for C++ deployment
â”‚   â”œâ”€â”€ run_experiments.py      # Batch experiment runner
â”‚   â””â”€â”€ submit_training.py      # HTCondor job generator
â”‚
â”œâ”€â”€ training/                    # HTCondor training jobs
â”‚   â”œâ”€â”€ README.md               # Job documentation
â”‚   â”œâ”€â”€ train_wrapper.sh        # Worker node script
â”‚   â”œâ”€â”€ jobs/                   # 29 .sub files for all experiments
â”‚   â””â”€â”€ logs/                   # Job output logs
â”‚
â”œâ”€â”€ trained_models/              # Output: trained model checkpoints
â”‚   â””â”€â”€ <experiment_name>/
â”‚       â”œâ”€â”€ best_model.pt
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ history.json        # â† All losses stored here
â”‚       â””â”€â”€ normalization.json
â”‚
â”œâ”€â”€ analysis/                    # Results analysis and visualization
â”‚   â”œâ”€â”€ README.md               # Analysis tools documentation
â”‚   â”œâ”€â”€ experiment_analysis.ipynb # â­ Main analysis notebook
â”‚   â”œâ”€â”€ model_analysis.ipynb    # Interactive analysis
â”‚   â”œâ”€â”€ analyze_models.py       # Analysis functions
â”‚   â”œâ”€â”€ physics_analysis.py     # Physics-specific tests
â”‚   â”œâ”€â”€ timing_benchmark.py     # â­ Timing benchmark tool
â”‚   â”œâ”€â”€ timing_comparison_plots.py # Timing visualizations
â”‚   â”œâ”€â”€ generate_paper_quality_plots.py # Publication-ready figures
â”‚   â”œâ”€â”€ trajectory_visualizer.py # Track visualization
â”‚   â”œâ”€â”€ run_analysis.py         # Batch analysis runner
â”‚   â”œâ”€â”€ results/                # Analysis outputs
â”‚   â””â”€â”€ plots/                  # Generated figures
â”‚
â”œâ”€â”€ benchmarking/                # C++ baseline benchmarks
â”‚   â”œâ”€â”€ benchmark_cpp.py        # Run C++ extrapolators
â”‚   â””â”€â”€ parse_benchmark_results.py # Parse benchmark logs
â”‚
â”œâ”€â”€ utils/                       # Utility modules
â”‚   â”œâ”€â”€ README.md               # Utils documentation
â”‚   â”œâ”€â”€ magnetic_field.py       # â­ Unified field map (InterpolatedFieldTorch)
â”‚   â””â”€â”€ rk4_propagator.py       # Python RK4 integrator
â”‚
â”œâ”€â”€ notes/                       # Documentation
â”‚   â”œâ”€â”€ experimental_protocol.tex  # Full experiment methodology
â”‚   â””â”€â”€ experimental_protocol.pdf
â”‚
â””â”€â”€ cluster/                     # HTCondor utilities
    â”œâ”€â”€ README.md
    â””â”€â”€ monitor_training.sh     # Job monitoring script
```

---

## Quick Reference

### Training Commands

```bash
# MLP EXPERIMENTS (data loss only)
python train.py --model mlp --preset tiny --name mlp_tiny_v1
python train.py --model mlp --preset small --name mlp_small_v1
python train.py --model mlp --preset medium --name mlp_medium_v1
python train.py --model mlp --preset wide --name mlp_wide_v1

# PINN EXPERIMENTS (data + physics loss)
# Default physics weights (Î»_pde=1.0, Î»_ic=1.0)
python train.py --model pinn --preset tiny --name pinn_tiny_v1
python train.py --model pinn --preset small --name pinn_small_v1
python train.py --model pinn --preset medium --name pinn_medium_v1
python train.py --model pinn --preset wide --name pinn_wide_v1

# Strong physics enforcement
python train.py --model pinn --preset medium --lambda_pde 10.0 --lambda_ic 10.0 --name pinn_medium_strong_v1

# Weak physics regularization
python train.py --model pinn --preset medium --lambda_pde 0.01 --lambda_ic 0.01 --name pinn_medium_weak_v1

# RK_PINN EXPERIMENTS (multi-stage physics)
python train.py --model rk_pinn --preset tiny --name rkpinn_tiny_v1
python train.py --model rk_pinn --preset small --name rkpinn_small_v1
python train.py --model rk_pinn --preset medium --name rkpinn_medium_v1
python train.py --model rk_pinn --preset wide --name rkpinn_wide_v1
```

### Analysis Commands

```bash
# After training, analyze results:
cd analysis

# Main analysis notebook (recommended)
jupyter notebook experiment_analysis.ipynb

# Command-line analysis
python analyze_models.py --checkpoint_dir ../trained_models
python timing_benchmark.py --models_dir ../trained_models

# Generate paper-quality plots
python generate_paper_quality_plots.py
```

### Current Status

| Component | Status | Notes |
|-----------|--------|-------|
| Data Generation | âœ… Complete | 50M tracks in `training_50M.npz` |
| Momentum Splits | âœ… Complete | Low/Mid/High-p datasets (10M each) |
| Training Script | âœ… Ready | `models/train.py` functional |
| Architectures | âœ… Ready | MLP, PINN, RK_PINN (using InterpolatedFieldTorch) |
| Loss Tracking | âœ… Ready | All losses stored in history.json |
| Evaluation | âœ… Ready | `models/evaluate.py` functional |
| **V1 Training** | âœ… Complete | 53 models (cluster 3880818), 10 epochs |
| **V2 Training** | âœ… Complete | 22 shallow-wide models (cluster 3891076), 20 epochs |
| Unified Runner | âœ… Ready | `run_all_experiments.py` |
| Analysis Notebook | âœ… Ready | `analysis/experiment_analysis.ipynb` |
| Experiment Protocol | âœ… Ready | `notes/experimental_protocol.pdf` |
| ONNX Export | âœ… Ready | `models/export_onnx.py` functional |

### Key Results Summary

**C++ Reference Baseline:** 2.50 Î¼s/track (CashKarp RK4, measured via TrackExtrapolatorTesterSOA)

**Best V1 Results:**
| Model Type | Best Model | Position Error (mm) | Time (Î¼s/track) | Speedup vs C++ |
|------------|-----------|---------------------|-----------------|----------------|
| MLP | mlp_tiny_v1 | 0.024 | 1.10 | 2.27Ã— faster |
| PINN | pinn_weak_v1 | 0.030 | 1.55 | 1.61Ã— faster |
| RK_PINN | rkpinn_coll5_v1 | 0.025 | 3.10 | 0.81Ã— (slower) |

**V2 Design Rationale:** Based on timing analysis:
- Depth â†” Time: weak correlation (r=0.37)
- Width â†” Time: moderate correlation (r=0.60)
- Parameters â†” Time: strong correlation (r=0.83)

V2 uses shallow (1-2 layers) + wide (256-1024 neurons) architectures to maximize speed.

### Recent Updates (January 2026)

1. **V1 Training Complete**: 53 models trained (MLP, PINN, RK_PINN variants)
2. **V2 Training Complete**: 22 shallow-wide models (optimized for speed)
3. **Correct Baseline Established**: C++ RK4 = 2.50 Î¼s/track (not 75 Î¼s)
4. **Analysis Updated**: All notebooks use correct reference timing
5. **Fixed PINN/RK_PINN field model**: Uses `InterpolatedFieldTorch` (real field map)

---

## References

- **Physics:** LHCb Tracking TDR, Lorentz force equations
- **PINN:** Raissi et al. "Physics-informed neural networks" (2019)
- **RK Methods:** Butcher "Numerical Methods for ODEs" (2008)
- **Field Map:** `twodip.rtf` - LHCb dipole field measurements

---

*Last updated: January 22, 2026*

\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{bm}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{xcolor}
% Note: algorithm, algpseudocode, and tikz packages not available on this system
% Pseudocode will be written in equation/align environments instead

\geometry{margin=2.5cm}

% Custom commands
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\qop}{q/p}
\newcommand{\clight}{c_{\text{light}}}
\newcommand{\Ldata}{\mathcal{L}_{\text{data}}}
\newcommand{\Lpde}{\mathcal{L}_{\text{PDE}}}
\newcommand{\Lic}{\mathcal{L}_{\text{IC}}}
\newcommand{\Ltotal}{\mathcal{L}_{\text{total}}}
\newcommand{\dv}[2]{\frac{d #1}{d #2}}
\newcommand{\pdv}[2]{\frac{\partial #1}{\partial #2}}

% Theorem environments
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{remark}{Remark}[section]

\title{Mathematical Foundations of Neural Network\\Track Extrapolation in LHCb}
\author{G. Scriven}
\date{January 2026}

\begin{document}

\maketitle

\begin{abstract}
This document presents the complete mathematical derivations for neural network-based track extrapolation in the LHCb detector. We cover the physics of charged particle motion in magnetic fields, derive the governing equations in z-parameterization, and develop three neural network architectures: Multi-Layer Perceptron (MLP), Physics-Informed Neural Network (PINN), and Runge-Kutta Physics-Informed Neural Network (RK-PINN). We also provide a comprehensive treatment of Runge-Kutta numerical integration theory that motivates the RK-PINN architecture.
\end{abstract}

\tableofcontents
\newpage

%=============================================================================
\section{Introduction}
%=============================================================================

Track reconstruction in the LHCb detector requires extrapolating particle trajectories through the magnetic field. The traditional approach uses numerical integration of the equations of motion (Runge-Kutta methods). We explore neural network alternatives that learn the extrapolation mapping directly from data or by incorporating physics constraints.

\subsection{Notation}

Throughout this document, we use the following notation:
\begin{itemize}
    \item $\vect{x} = (x, y, z)^T$ - spatial position in mm
    \item $\vect{p} = (p_x, p_y, p_z)^T$ - momentum in MeV
    \item $\vect{B} = (B_x, B_y, B_z)^T$ - magnetic field in Tesla
    \item $t_x = \dv{x}{z}$, $t_y = \dv{y}{z}$ - track slopes (dimensionless)
    \item $\qop$ - charge over momentum in MeV$^{-1}$
    \item $\clight = 2.99792458 \times 10^{-4}$ - speed of light factor
\end{itemize}

%=============================================================================
\section{Physics of Charged Particle Motion}
%=============================================================================

\subsection{The Lorentz Force}

A charged particle with charge $q$ and velocity $\vect{v}$ in a magnetic field $\vect{B}$ experiences the Lorentz force:
\begin{equation}
    \vect{F} = q(\vect{v} \times \vect{B})
    \label{eq:lorentz}
\end{equation}

Using Newton's second law and the relativistic momentum $\vect{p} = \gamma m \vect{v}$:
\begin{equation}
    \dv{\vect{p}}{t} = q(\vect{v} \times \vect{B})
\end{equation}

\begin{remark}
The Lorentz force is perpendicular to the velocity, so it does no work on the particle. Energy (and thus $|\vect{p}|$) is conserved in a pure magnetic field.
\end{remark}

\subsection{Z-Parameterization}

In LHCb, tracks propagate predominantly in the $+z$ direction. It is therefore convenient to parameterize the trajectory by $z$ rather than time. We define the \textbf{track state} at position $z$ as:
\begin{equation}
    \vect{y}(z) = \begin{pmatrix} x(z) \\ y(z) \\ t_x(z) \\ t_y(z) \end{pmatrix}
\end{equation}

where $t_x = \dv{x}{z}$ and $t_y = \dv{y}{z}$ are the track slopes.

The relationship between time and $z$ derivatives is:
\begin{equation}
    \dv{}{t} = \frac{v_z}{1} \dv{}{z} = \frac{p_z}{\gamma m} \dv{}{z}
\end{equation}

\subsection{Derivation of the Equations of Motion}

\begin{theorem}[Equations of Motion in Z-Parameterization]
The evolution of the track state $\vect{y}(z)$ is governed by:
\begin{align}
    \dv{x}{z} &= t_x \label{eq:dxdz} \\
    \dv{y}{z} &= t_y \label{eq:dydz} \\
    \dv{t_x}{z} &= \kappa \cdot N \cdot \left[ t_x t_y B_x - (1 + t_x^2) B_y + t_y B_z \right] \label{eq:dtxdz} \\
    \dv{t_y}{z} &= \kappa \cdot N \cdot \left[ (1 + t_y^2) B_x - t_x t_y B_y - t_x B_z \right] \label{eq:dtydz}
\end{align}
where:
\begin{equation}
    \kappa = \frac{q}{p} \cdot \clight, \qquad N = \sqrt{1 + t_x^2 + t_y^2}
\end{equation}
\end{theorem}

\begin{proof}
We derive the slope equations starting from the Lorentz force. The momentum components are related to the slopes by:
\begin{equation}
    p_x = p \cdot \frac{t_x}{N}, \quad p_y = p \cdot \frac{t_y}{N}, \quad p_z = p \cdot \frac{1}{N}
\end{equation}
where $N = \sqrt{1 + t_x^2 + t_y^2}$ ensures $|\vect{p}| = p$.

The velocity is $\vect{v} = \vect{p}/(\gamma m)$, so:
\begin{equation}
    v_z = \frac{p_z}{\gamma m} = \frac{p}{N \gamma m}
\end{equation}

From the Lorentz force $\dv{\vect{p}}{t} = q(\vect{v} \times \vect{B})$, the $x$-component is:
\begin{equation}
    \dv{p_x}{t} = q(v_y B_z - v_z B_y) = \frac{q}{\gamma m} (p_y B_z - p_z B_y)
\end{equation}

Converting to $z$-derivatives using $\dv{t} = v_z \dv{z}$:
\begin{equation}
    \dv{p_x}{z} = \frac{1}{v_z} \dv{p_x}{t} = \frac{N \gamma m}{p} \cdot \frac{q}{\gamma m} (p_y B_z - p_z B_y)
\end{equation}

Simplifying:
\begin{equation}
    \dv{p_x}{z} = \frac{qN}{p} \left( \frac{p t_y}{N} B_z - \frac{p}{N} B_y \right) = q(t_y B_z - B_y)
\end{equation}

Now, $t_x = p_x / p_z = p_x N / p$, so:
\begin{equation}
    \dv{t_x}{z} = \frac{N}{p} \dv{p_x}{z} + \frac{p_x}{p} \dv{N}{z}
\end{equation}

Since $|\vect{p}|$ is conserved and $N$ depends on slopes:
\begin{equation}
    \dv{N}{z} = \frac{t_x}{N} \dv{t_x}{z} + \frac{t_y}{N} \dv{t_y}{z}
\end{equation}

After algebraic manipulation (similar derivation for $t_y$), we obtain:
\begin{align}
    \dv{t_x}{z} &= \frac{q}{p} \clight \cdot N \cdot \left[ t_x t_y B_x - (1 + t_x^2) B_y + t_y B_z \right] \\
    \dv{t_y}{z} &= \frac{q}{p} \clight \cdot N \cdot \left[ (1 + t_y^2) B_x - t_x t_y B_y - t_x B_z \right]
\end{align}
where $\clight = 2.99792458 \times 10^{-4}$ accounts for unit conversions.
\end{proof}

\subsection{Compact Form}

We can write the system compactly as:
\begin{equation}
    \dv{\vect{y}}{z} = \vect{f}(\vect{y}, z; \qop)
    \label{eq:ode_system}
\end{equation}
where $\vect{f}: \mathbb{R}^4 \times \mathbb{R} \times \mathbb{R} \to \mathbb{R}^4$ is defined by Equations~\eqref{eq:dxdz}--\eqref{eq:dtydz}.

%=============================================================================
\section{The LHCb Magnetic Field}
%=============================================================================

\subsection{Dipole Magnet Characteristics}

The LHCb dipole magnet produces a field primarily in the $y$-direction (vertical):
\begin{itemize}
    \item Peak field: $|B_y| \approx 1.03$ T at $z \approx 5000$ mm
    \item Field integral: $\int B_y \, dz \approx 4.44$ T$\cdot$m
    \item Dominant component: $B_y$ (causes bending in the $x$-$z$ plane)
\end{itemize}

\subsection{Field Map Representation}

\subsubsection{General Formulation}

Throughout this work, we treat the magnetic field as a \textbf{callable function}:
\begin{equation}
    \vect{B}: \mathbb{R}^3 \to \mathbb{R}^3, \quad (x, y, z) \mapsto (B_x, B_y, B_z)
\end{equation}

This function may be implemented as:
\begin{enumerate}
    \item An \textbf{analytical approximation} (e.g., Gaussian profile)
    \item A \textbf{tabulated field map} with interpolation
    \item A \textbf{neural network surrogate} trained on field map data
\end{enumerate}

For physics-informed training, the field function must be \textbf{differentiable} with respect to position to support automatic differentiation.

\subsubsection{Tabulated Field Map (twodip.rtf)}

The LHCb field map is provided as a tabulated 3D grid:
\begin{itemize}
    \item Format: $(x, y, z, B_x, B_y, B_z)$ values on regular grid
    \item Grid extent: $x \in [-4000, 4000]$ mm, $y \in [-4000, 4000]$ mm, $z \in [-500, 14000]$ mm
    \item Grid spacing: $\Delta x = \Delta y = 100$ mm, $\Delta z = 100$ mm
    \item Total points: $\approx 958,000$
\end{itemize}

Field values at arbitrary positions are obtained via \textbf{trilinear interpolation}:
\begin{equation}
    B_i(x, y, z) = \sum_{\alpha, \beta, \gamma \in \{0,1\}} w_\alpha(x) \, w_\beta(y) \, w_\gamma(z) \, B_i^{(\alpha\beta\gamma)}
\end{equation}
where $w_0(\xi) = 1 - \frac{\xi - \xi_j}{\Delta\xi}$ and $w_1(\xi) = \frac{\xi - \xi_j}{\Delta\xi}$ are linear interpolation weights.

\subsubsection{Interpolation Error Analysis}

For trilinear interpolation on a grid with spacing $h$, the local truncation error is:
\begin{equation}
    \epsilon_{\text{interp}} = \mathcal{O}(h^2 \cdot \max|\partial^2 B_i / \partial x_j^2|)
\end{equation}

For the LHCb field map with $h = 100$ mm and typical field curvature:
\begin{equation}
    \epsilon_{\text{interp}} \lesssim 10^{-5} \text{ T}
\end{equation}

This is negligible compared to the peak field of $\sim 1$ T.

\begin{remark}[Differentiability of Interpolated Field]
Trilinear interpolation is \textbf{piecewise linear} and therefore \textbf{not continuously differentiable} at grid cell boundaries. The gradient is discontinuous at these points:
\begin{equation}
    \pdv{B_i}{x_j} \text{ has jump discontinuities at } x_j = n \cdot \Delta x_j
\end{equation}

For PINN training, this means:
\begin{itemize}
    \item The physics residual may have small discontinuities at cell boundaries
    \item Using many collocation points averages over these discontinuities
    \item Alternative: use cubic spline or higher-order interpolation for $C^1$ continuity
\end{itemize}

In practice, with 10--20 random collocation points per sample, the effect is negligible.
\end{remark}

\subsection{Gaussian Field Approximation (Optional)}

For rapid prototyping or when the field map is unavailable, we can use a Gaussian approximation:
\begin{equation}
    B_y(z) = B_0 \exp\left( -\frac{1}{2} \left( \frac{z - z_c}{\sigma_z} \right)^2 \right)
    \label{eq:gaussian_field}
\end{equation}
with fitted parameters:
\begin{equation}
    B_0 = -1.0182 \text{ T}, \quad z_c = 5007 \text{ mm}, \quad \sigma_z = 1744 \text{ mm}
\end{equation}

\textbf{Approximation error:} The Gaussian model has RMS error $\approx 0.013$ T ($\sim 1.3\%$ of peak) when compared to the full field map along the beam axis. This error propagates to trajectory predictions and should be accounted for in error budgets.

\begin{remark}[When to Use Each Field Model]
\begin{itemize}
    \item \textbf{Tabulated field map}: For production training and final evaluation. Provides highest accuracy.
    \item \textbf{Gaussian approximation}: For initial prototyping, debugging, and when field map access is restricted. Faster to evaluate.
\end{itemize}

Our training data is generated using the Gaussian approximation for simplicity, but the PINN can be retrained with the full field map for maximum accuracy.
\end{remark}

%=============================================================================
\section{Multi-Layer Perceptron (MLP)}
%=============================================================================

\subsection{Architecture}

The MLP learns the extrapolation mapping directly from data without explicit physics. Given input features:
\begin{equation}
    \vect{x} = (x_0, y_0, t_{x,0}, t_{y,0}, \qop, \Delta z)^T \in \mathbb{R}^6
\end{equation}
the network predicts the final state:
\begin{equation}
    \hat{\vect{y}} = (x_f, y_f, t_{x,f}, t_{y,f})^T \in \mathbb{R}^4
\end{equation}

\subsection{Network Definition}

\begin{definition}[Multi-Layer Perceptron]
An MLP with $L$ hidden layers is defined as:
\begin{equation}
    \text{MLP}(\vect{x}) = \mat{W}_{L+1} \sigma_L(\mat{W}_L \sigma_{L-1}(\cdots \sigma_1(\mat{W}_1 \vect{x} + \vect{b}_1) \cdots) + \vect{b}_L) + \vect{b}_{L+1}
\end{equation}
where $\mat{W}_i \in \mathbb{R}^{d_i \times d_{i-1}}$ are weight matrices, $\vect{b}_i$ are bias vectors, and $\sigma_i$ are activation functions.
\end{definition}

\subsection{Input/Output Normalization}

For stable training, we apply z-score normalization:
\begin{equation}
    \tilde{x}_i = \frac{x_i - \mu_i}{\sigma_i}
\end{equation}
where $\mu_i$ and $\sigma_i$ are the mean and standard deviation computed from training data.

The network operates on normalized inputs and outputs:
\begin{equation}
    \hat{\vect{y}} = \sigma_y \odot \text{MLP}(\tilde{\vect{x}}) + \mu_y
\end{equation}

\subsection{Loss Function}

The MLP is trained with Mean Squared Error (MSE) loss:
\begin{equation}
    \Ldata = \frac{1}{N} \sum_{i=1}^{N} \|\hat{\vect{y}}_i - \vect{y}_i^*\|^2
    \label{eq:mse_loss}
\end{equation}
where $\vect{y}_i^*$ is the ground truth from Runge-Kutta integration.

\subsection{Implicit Physics Learning}

\begin{remark}
The MLP learns the physics \textit{implicitly} from training data generated by the RK extrapolator. The network approximates the flow map:
\begin{equation}
    \Phi_{\Delta z}: \vect{y}_0 \mapsto \vect{y}(\Delta z)
\end{equation}
which is the solution operator for the ODE system~\eqref{eq:ode_system}.
\end{remark}

%=============================================================================
\section{Physics-Informed Neural Network (PINN)}
%=============================================================================

\subsection{Concept}

Physics-Informed Neural Networks (PINNs) incorporate the governing physics equations directly into the loss function. Instead of learning only from data, the network is constrained to satisfy the differential equations.

\subsection{Key Distinction from MLP}

\begin{remark}[MLP vs PINN: Fundamental Difference]
The MLP performs a \textbf{direct mapping}:
\begin{equation}
    \text{MLP}: (x_0, y_0, t_{x,0}, t_{y,0}, q/p, z_0, z_{\text{end}}) \mapsto (x_{\text{end}}, y_{\text{end}}, t_{x,\text{end}}, t_{y,\text{end}})
\end{equation}
One forward pass produces one answer. There is no notion of the path taken between $z_0$ and $z_{\text{end}}$.

The PINN, in contrast, outputs a \textbf{continuous trajectory} parameterized by $\zeta \in [0,1]$:
\begin{equation}
    \text{PINN}: (x_0, y_0, t_{x,0}, t_{y,0}, q/p, z_0, z_{\text{end}}, \zeta) \mapsto (x, y, t_x, t_y) \text{ at that } \zeta
\end{equation}
where:
\begin{itemize}
    \item $\zeta = 0$ corresponds to the initial state at $z_0$
    \item $\zeta = 1$ corresponds to the final state at $z_{\text{end}}$
    \item $\zeta = 0.5$ corresponds to the state halfway through the propagation
\end{itemize}

This trajectory representation is essential because the physics loss requires computing derivatives \textit{along} the trajectory.
\end{remark}

\subsection{Network as Trajectory Function}

The PINN learns the continuous trajectory:
\begin{equation}
    \vect{y}_\theta: [0, 1] \to \mathbb{R}^4, \quad \zeta \mapsto \vect{y}_\theta(\zeta)
\end{equation}
where $\zeta = (z - z_0)/\Delta z \in [0, 1]$ is the normalized position and $\theta$ represents network parameters.

\subsection{Training Data Requirements}

\begin{remark}[No Intermediate Trajectory Data Required]
A common misconception is that PINNs require ground truth trajectories for training. This is \textbf{not} the case.

\textbf{Training data needed:}
\begin{itemize}
    \item Initial state: $(x_0, y_0, t_{x,0}, t_{y,0}, q/p)$ at $z_0$
    \item Final state: $(x_{\text{end}}, y_{\text{end}}, t_{x,\text{end}}, t_{y,\text{end}})$ at $z_{\text{end}}$
\end{itemize}

This is \textbf{identical} to the MLP training data. No intermediate trajectory points are needed.

\textbf{How it works:} The physics loss (Eq.~\ref{eq:pde_loss}) does not compare the network's intermediate predictions against any ground truth. Instead, it checks whether the \textit{network's own predicted trajectory} satisfies the Lorentz force equations. The network learns the correct intermediate states by being forced to satisfy physics self-consistently.

The only external knowledge required beyond endpoint data is:
\begin{enumerate}
    \item The magnetic field model $\vect{B}(z)$ (known analytically)
    \item The Lorentz force equations (known from physics)
\end{enumerate}
\end{remark}

\subsection{Loss Function Components}

The total PINN loss consists of three components:
\begin{equation}
    \Ltotal = \Ldata + \lambda_{\text{IC}} \Lic + \lambda_{\text{PDE}} \Lpde
    \label{eq:pinn_loss}
\end{equation}

\subsubsection{Data Loss}

At the endpoint $\zeta = 1$:
\begin{equation}
    \Ldata = \frac{1}{N} \sum_{i=1}^{N} \|\vect{y}_\theta(\zeta=1; \vect{x}_i) - \vect{y}_i^*\|^2
\end{equation}

\subsubsection{Initial Condition Loss}

At $\zeta = 0$, the trajectory must match the initial state:
\begin{equation}
    \Lic = \frac{1}{N} \sum_{i=1}^{N} \|\vect{y}_\theta(\zeta=0; \vect{x}_i) - \vect{y}_{0,i}\|^2
\end{equation}
where $\vect{y}_{0,i} = (x_{0,i}, y_{0,i}, t_{x,0,i}, t_{y,0,i})^T$.

\subsubsection{PDE Residual Loss}

The physics constraint is enforced at \textit{collocation points} $\{\zeta_k\}_{k=1}^K$ sampled along the trajectory:
\begin{equation}
    \Lpde = \frac{1}{NK} \sum_{i=1}^{N} \sum_{k=1}^{K} \left\| \dv{\vect{y}_\theta}{\zeta}\bigg|_{\zeta_k} - \Delta z \cdot \vect{f}(\vect{y}_\theta(\zeta_k), z_k; \qop_i) \right\|^2
    \label{eq:pde_loss}
\end{equation}

\subsection{Why Collocation Points Throughout the Trajectory?}

\begin{remark}[Endpoints Alone Are Insufficient]
One might ask: is it sufficient to enforce the physics only at the endpoints ($\zeta = 0$ and $\zeta = 1$)?

The answer is \textbf{no}, and the LHCb magnetic field illustrates why. The field varies significantly along $z$:
\begin{align}
    z = 2500 \text{ mm}: \quad & B_y \approx 0 \text{ (entering magnet)} \\
    z = 5000 \text{ mm}: \quad & B_y \approx -1.0 \text{ T (peak field)} \\
    z = 7500 \text{ mm}: \quad & B_y \approx 0 \text{ (exiting magnet)}
\end{align}

If we only enforced physics at the endpoints where $B \approx 0$, the network could learn a \textbf{straight-line trajectory}:
\begin{itemize}
    \item At $\zeta = 0$: $d\vect{y}/d\zeta \approx 0$ (\checkmark satisfies physics, no field)
    \item At $\zeta = 1$: $d\vect{y}/d\zeta \approx 0$ (\checkmark satisfies physics, no field)
    \item At $\zeta = 0.5$: Completely ignores the strong bending from $B_y = -1.0$ T
\end{itemize}

By sampling collocation points \textbf{throughout} $[0, 1]$, we force the network to learn a trajectory that respects the Lorentz force \textit{everywhere}, including where the field is strongest. The trajectory ``discovers'' the correct curved path by satisfying the physics constraints at many intermediate points.
\end{remark}

\subsection{Automatic Differentiation}

The key insight of PINNs is that the derivative $\pdv{\vect{y}_\theta}{\zeta}$ can be computed exactly using automatic differentiation:
\begin{equation}
    \pdv{y_j}{\zeta} = \pdv{}{\zeta} \text{NN}_j(\vect{x}, \zeta; \theta)
\end{equation}
This is computed via backpropagation through the network graph.

\subsection{Concrete Example of Physics Enforcement}

To illustrate concretely how the physics is enforced without trajectory data, consider a collocation point at $\zeta = 0.5$ (middle of the magnet, $z \approx 5000$ mm):

\begin{enumerate}
    \item \textbf{Network prediction:} Query $\vect{y}_\theta(\zeta=0.5)$, suppose it returns $(x, y, t_x, t_y) = (10.2, 3.1, 0.15, 0.02)$
    
    \item \textbf{Autodiff derivative:} Compute $d\vect{y}_\theta/d\zeta$ at $\zeta=0.5$ via backpropagation, suppose it gives $(0.15, 0.02, -0.003, 0.0001)$
    
    \item \textbf{Field lookup:} At $z = 5000$ mm, the field model gives $B_y = -1.0$ T
    
    \item \textbf{Physics calculation:} Using the Lorentz force equations~\eqref{eq:dtxdz}--\eqref{eq:dtydz} with the network's predicted state and the known field, compute what $d\vect{y}/d\zeta$ \textit{should} be: $(0.15, 0.02, -0.0028, 0.0001)$
    
    \item \textbf{Residual:} $\| (0.15, 0.02, -0.003, 0.0001) - (0.15, 0.02, -0.0028, 0.0001) \|^2$ penalizes the mismatch
\end{enumerate}

Note that step 4 uses only the magnetic field model and physics equations---no ground truth intermediate state is required. The network is trained to make its own predictions self-consistent with physics.

\subsection{Algorithm}

The PINN training procedure follows these steps:

\begin{enumerate}
    \item \textbf{Input:} Batch of initial states $\{\vect{x}_i\}$, ground truth endpoints $\{\vect{y}_i^*\}$
    \item Sample collocation points $\{\zeta_k\}_{k=1}^K \sim \text{Uniform}(0, 1)$
    \item For each sample $i$:
    \begin{itemize}
        \item Compute $\vect{y}_\theta(\zeta=0)$, $\vect{y}_\theta(\zeta=1)$, and $\vect{y}_\theta(\zeta_k)$ for all $k$
        \item Compute $\pdv{\vect{y}_\theta}{\zeta}$ at each $\zeta_k$ via autodiff
        \item Evaluate physics residual using Eqs.~\eqref{eq:dxdz}--\eqref{eq:dtydz}
    \end{itemize}
    \item Compute $\Ltotal$ using Eq.~\eqref{eq:pinn_loss}
    \item Update $\theta$ via gradient descent
\end{enumerate}

\textbf{At inference time:} Simply evaluate the network at $\zeta = 1$ to obtain the final state, identical to MLP inference. The intermediate trajectory capability is only used during training.

%=============================================================================
\section{Runge-Kutta Numerical Integration}
%=============================================================================

\subsection{General Framework}

Runge-Kutta methods approximate the solution of an initial value problem:
\begin{equation}
    \dv{\vect{y}}{z} = \vect{f}(\vect{y}, z), \quad \vect{y}(z_0) = \vect{y}_0
\end{equation}

\begin{definition}[Explicit Runge-Kutta Method]
An $s$-stage explicit RK method advances the solution from $\vect{y}_n$ to $\vect{y}_{n+1}$ over step $h$ by:
\begin{align}
    \vect{k}_1 &= h \cdot \vect{f}(z_n, \vect{y}_n) \\
    \vect{k}_2 &= h \cdot \vect{f}(z_n + c_2 h, \vect{y}_n + a_{21} \vect{k}_1) \\
    \vect{k}_3 &= h \cdot \vect{f}(z_n + c_3 h, \vect{y}_n + a_{31} \vect{k}_1 + a_{32} \vect{k}_2) \\
    &\vdots \\
    \vect{k}_s &= h \cdot \vect{f}(z_n + c_s h, \vect{y}_n + \sum_{j=1}^{s-1} a_{sj} \vect{k}_j) \\
    \vect{y}_{n+1} &= \vect{y}_n + \sum_{i=1}^{s} b_i \vect{k}_i
\end{align}
\end{definition}

\subsection{Butcher Tableau}

The RK method is characterized by its Butcher tableau:
\begin{equation}
\begin{array}{c|cccc}
c_1 & 0 & & & \\
c_2 & a_{21} & 0 & & \\
c_3 & a_{31} & a_{32} & 0 & \\
\vdots & \vdots & & \ddots & \\
c_s & a_{s1} & a_{s2} & \cdots & 0 \\
\hline
& b_1 & b_2 & \cdots & b_s
\end{array}
\end{equation}

\subsection{Classical RK4}

The classical fourth-order Runge-Kutta method (RK4) has the tableau:
\begin{equation}
\begin{array}{c|cccc}
0 & & & & \\
\frac{1}{2} & \frac{1}{2} & & & \\
\frac{1}{2} & 0 & \frac{1}{2} & & \\
1 & 0 & 0 & 1 & \\
\hline
& \frac{1}{6} & \frac{1}{3} & \frac{1}{3} & \frac{1}{6}
\end{array}
\end{equation}

Explicitly:
\begin{align}
    \vect{k}_1 &= h \cdot \vect{f}(z_n, \vect{y}_n) \\
    \vect{k}_2 &= h \cdot \vect{f}\left(z_n + \frac{h}{2}, \vect{y}_n + \frac{\vect{k}_1}{2}\right) \\
    \vect{k}_3 &= h \cdot \vect{f}\left(z_n + \frac{h}{2}, \vect{y}_n + \frac{\vect{k}_2}{2}\right) \\
    \vect{k}_4 &= h \cdot \vect{f}(z_n + h, \vect{y}_n + \vect{k}_3) \\
    \vect{y}_{n+1} &= \vect{y}_n + \frac{1}{6}(\vect{k}_1 + 2\vect{k}_2 + 2\vect{k}_3 + \vect{k}_4)
    \label{eq:rk4}
\end{align}

\subsection{Order Conditions}

\begin{theorem}[Local Truncation Error]
The local truncation error of RK4 is $O(h^5)$, making it a fourth-order method with global error $O(h^4)$.
\end{theorem}

The order conditions are derived by matching Taylor series coefficients. For order $p$, the method must satisfy:
\begin{equation}
    \vect{y}(z_n + h) - \vect{y}_{n+1} = O(h^{p+1})
\end{equation}

\subsection{Geometric Interpretation}

RK4 can be interpreted as:
\begin{enumerate}
    \item $\vect{k}_1$: Slope at the starting point
    \item $\vect{k}_2$: Slope at the midpoint using $\vect{k}_1$
    \item $\vect{k}_3$: Slope at the midpoint using $\vect{k}_2$
    \item $\vect{k}_4$: Slope at the endpoint using $\vect{k}_3$
\end{enumerate}

The final update is a weighted average: $(1 + 2 + 2 + 1)/6 = 1$.

%=============================================================================
\section{RK-PINN: Runge-Kutta Physics-Informed Neural Network}
%=============================================================================

\subsection{Motivation}

The RK-PINN combines the structure of Runge-Kutta methods with neural network learning. The key insight is that the intermediate stages of RK methods provide natural positions for physics constraints.

\subsection{Architecture}

\begin{definition}[RK-PINN]
The RK-PINN consists of:
\begin{enumerate}
    \item \textbf{Shared backbone} $\vect{h}_\theta: \mathbb{R}^6 \to \mathbb{R}^d$ that extracts features
    \item \textbf{Stage heads} $\{g_{\phi_i}\}_{i=1}^s$ that predict states at stage positions
    \item \textbf{Learnable weights} $\{w_i\}_{i=1}^s$ for combining stage outputs
\end{enumerate}
\end{definition}

For a 4-stage RK-PINN (analogous to RK4):
\begin{align}
    \vect{h} &= \text{Backbone}(\vect{x}) \\
    \hat{\vect{y}}_1 &= g_{\phi_1}(\vect{h}, \zeta = 0.25) \quad \text{(at } z_0 + 0.25\Delta z \text{)} \\
    \hat{\vect{y}}_2 &= g_{\phi_2}(\vect{h}, \zeta = 0.50) \quad \text{(at } z_0 + 0.50\Delta z \text{)} \\
    \hat{\vect{y}}_3 &= g_{\phi_3}(\vect{h}, \zeta = 0.75) \quad \text{(at } z_0 + 0.75\Delta z \text{)} \\
    \hat{\vect{y}}_4 &= g_{\phi_4}(\vect{h}, \zeta = 1.00) \quad \text{(at } z_0 + \Delta z \text{)} \\
    \hat{\vect{y}}_{\text{final}} &= \sum_{i=1}^{4} \tilde{w}_i \hat{\vect{y}}_i
\end{align}
where $\tilde{w}_i = \text{softmax}(\vect{w})_i$ ensures weights sum to 1.

\subsection{Weight Initialization}

The weights are initialized to match RK4:
\begin{equation}
    w_1^{(0)} = \frac{1}{6}, \quad w_2^{(0)} = \frac{2}{6}, \quad w_3^{(0)} = \frac{2}{6}, \quad w_4^{(0)} = \frac{1}{6}
\end{equation}

During training, these weights can adapt to better fit the data.

\subsection{Loss Function}

The RK-PINN loss combines data and physics terms:
\begin{equation}
    \Ltotal = \Ldata + \lambda_{\text{IC}} \Lic + \lambda_{\text{PDE}} \sum_{i=1}^{s} \mathcal{L}_{\text{PDE}}^{(i)}
\end{equation}

where the PDE loss at each stage is:
\begin{equation}
    \mathcal{L}_{\text{PDE}}^{(i)} = \frac{1}{N} \sum_{j=1}^{N} \left\| \pdv{\hat{\vect{y}}_i}{\zeta} - \Delta z \cdot \vect{f}(\hat{\vect{y}}_i, z_i; \qop_j) \right\|^2
\end{equation}

\subsection{Comparison with Standard PINN}

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Property} & \textbf{PINN} & \textbf{RK-PINN} \\
\midrule
Collocation points & Random sampling & Fixed at RK positions \\
Architecture & Single network & Backbone + stage heads \\
Stage weights & N/A & Learnable (init: RK4) \\
Physics structure & Soft constraint & Structured like RK \\
\bottomrule
\end{tabular}
\caption{Comparison of PINN and RK-PINN architectures.}
\end{table}

%=============================================================================
\section{Summary of Loss Functions}
%=============================================================================

\subsection{MLP}
\begin{equation}
    \mathcal{L}_{\text{MLP}} = \frac{1}{N} \sum_{i=1}^{N} \|\hat{\vect{y}}_i - \vect{y}_i^*\|^2
\end{equation}

\subsection{PINN}
\begin{equation}
    \mathcal{L}_{\text{PINN}} = \underbrace{\frac{1}{N} \sum_{i=1}^{N} \|\hat{\vect{y}}_i - \vect{y}_i^*\|^2}_{\Ldata} + \lambda_{\text{IC}} \underbrace{\frac{1}{N} \sum_{i=1}^{N} \|\hat{\vect{y}}_{0,i} - \vect{y}_{0,i}\|^2}_{\Lic} + \lambda_{\text{PDE}} \underbrace{\frac{1}{NK} \sum_{i,k} \|\mathcal{R}_{ik}\|^2}_{\Lpde}
\end{equation}
where the residual is:
\begin{equation}
    \mathcal{R}_{ik} = \pdv{\vect{y}_\theta}{\zeta}\bigg|_{\zeta_k} - \Delta z \cdot \vect{f}(\vect{y}_\theta(\zeta_k), z_k; \qop_i)
\end{equation}

\subsection{RK-PINN}
\begin{equation}
    \mathcal{L}_{\text{RK-PINN}} = \Ldata + \lambda_{\text{IC}} \Lic + \lambda_{\text{PDE}} \sum_{s=1}^{4} \mathcal{L}_{\text{PDE}}^{(s)}
\end{equation}

%=============================================================================
\section{Implementation Notes}
%=============================================================================

\subsection{Critical Constants}

\begin{equation}
    \clight = 2.99792458 \times 10^{-4} \quad \text{[mm/ns $\times$ unit conversions]}
\end{equation}

This constant converts:
\begin{equation}
    \kappa = \frac{q}{p} \cdot \clight \quad \text{[MeV}^{-1} \text{ T}^{-1} \text{ mm}^{-1}\text{]}
\end{equation}

\subsection{Normalization Factor}

The path length factor:
\begin{equation}
    N = \sqrt{1 + t_x^2 + t_y^2} = \frac{|\vect{p}|}{p_z} = \frac{ds}{dz}
\end{equation}
accounts for the fact that the particle travels a distance $ds = N \cdot dz$ when advancing by $dz$ in $z$.

\subsection{Dominant Field Component}

In LHCb, $B_y$ dominates, causing bending in the $x$-$z$ plane:
\begin{equation}
    \dv{t_x}{z} \approx -\kappa \cdot N \cdot (1 + t_x^2) \cdot B_y
\end{equation}

For small slopes ($t_x, t_y \ll 1$) and positive particles ($q > 0$) in a negative field ($B_y < 0$):
\begin{equation}
    \dv{t_x}{z} > 0 \quad \Rightarrow \quad \text{track bends to positive } x
\end{equation}

%=============================================================================
\section{Error Analysis and Uncertainty Quantification}
%=============================================================================

A rigorous understanding of the error sources is essential for deploying neural network extrapolators in a physics experiment. We categorize errors into several distinct types.

%-----------------------------------------------------------------------------
\subsection{Training Data Errors}
%-----------------------------------------------------------------------------

\subsubsection{Ground Truth Generation Error}

The training targets are generated using numerical integration (RK4). The local truncation error of RK4 is:
\begin{equation}
    \epsilon_{\text{RK4}}^{\text{local}} = \mathcal{O}(h^5)
\end{equation}
where $h$ is the step size. For $n$ steps, the global error accumulates:
\begin{equation}
    \epsilon_{\text{RK4}}^{\text{global}} = \mathcal{O}(h^4)
\end{equation}

With our step size $h = 10$ mm over $\Delta z \approx 2500$ mm ($n = 250$ steps), the RK4 error is typically $\mathcal{O}(10^{-8})$ in normalized coordinates, well below our target precision.

\subsubsection{Magnetic Field Evaluation Error}

The magnetic field function $\vect{B}(x, y, z)$ introduces error depending on its implementation:

\paragraph{Option 1: Interpolated Field Map (Recommended)}

When using trilinear interpolation from the tabulated field map:
\begin{equation}
    \epsilon_{\text{field}}^{\text{interp}} = \mathcal{O}(h_{\text{grid}}^2) \approx 10^{-5} \text{ T}
\end{equation}
where $h_{\text{grid}} = 100$ mm is the field map grid spacing.

This error is \textbf{negligible} compared to the peak field ($\sim 1$ T) and does not significantly impact trajectory accuracy.

\paragraph{Option 2: Gaussian Approximation}

When using the analytical Gaussian model:
\begin{equation}
    B_y^{\text{approx}}(z) = B_0 \exp\left(-\frac{(z - z_c)^2}{2\sigma_z^2}\right)
\end{equation}

The approximation error relative to the true field map is:
\begin{equation}
    \epsilon_{\text{field}}^{\text{Gaussian}}(z) = B_y^{\text{true}}(z) - B_y^{\text{approx}}(z)
\end{equation}
with RMS $\approx 0.013$ T (about 1.3\% of peak field).

This propagates to trajectory error as:
\begin{equation}
    \delta t_x \sim \kappa \int_{z_0}^{z_{\text{end}}} \epsilon_{\text{field}}(z) \, dz
\end{equation}

For high-precision applications, the interpolated field map should be used.

\paragraph{Comparison of Field Models}

\begin{table}[ht]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Field Model} & \textbf{Error Magnitude} & \textbf{Differentiable?} \\
\midrule
Tabulated + trilinear interp. & $\sim 10^{-5}$ T & Piecewise (discontinuous gradient) \\
Tabulated + cubic spline & $\sim 10^{-7}$ T & $C^1$ continuous \\
Gaussian approximation & $\sim 10^{-2}$ T & $C^\infty$ (smooth) \\
\bottomrule
\end{tabular}
\caption{Comparison of magnetic field model accuracy and differentiability.}
\end{table}

\subsubsection{Sampling Bias}

If the training distribution $P_{\text{train}}(\vect{x})$ does not match the operational distribution $P_{\text{physics}}(\vect{x})$, systematic biases emerge:
\begin{equation}
    \text{Bias} = \mathbb{E}_{P_{\text{physics}}}[\hat{y}(\vect{x})] - \mathbb{E}_{P_{\text{physics}}}[y^*(\vect{x})]
\end{equation}

Critical distributions to match:
\begin{itemize}
    \item Momentum spectrum (especially low-$p$ tails)
    \item Angular distribution ($t_x$, $t_y$)
    \item Charge ratio ($q/p$ sign)
    \item Spatial distribution ($x$, $y$ at entry)
\end{itemize}

%-----------------------------------------------------------------------------
\subsection{Model Approximation Errors}
%-----------------------------------------------------------------------------

\subsubsection{Network Capacity Error}

A neural network with finite parameters $\theta$ cannot represent arbitrary functions. The approximation error is:
\begin{equation}
    \epsilon_{\text{approx}} = \inf_{\theta} \mathbb{E}\left[ \| f_\theta(\vect{x}) - f^*(\vect{x}) \|^2 \right]
\end{equation}

This depends on:
\begin{itemize}
    \item Network depth $L$ and width $W$
    \item Activation function smoothness
    \item Complexity of the true mapping $f^*$
\end{itemize}

Universal approximation theorems guarantee $\epsilon_{\text{approx}} \to 0$ as capacity $\to \infty$, but practical networks have non-zero error.

\subsubsection{Optimization Error}

Gradient descent may not find the global optimum:
\begin{equation}
    \epsilon_{\text{opt}} = \mathcal{L}(\theta_{\text{final}}) - \mathcal{L}(\theta^*)
\end{equation}
where $\theta^*$ is the global minimum. Sources include:
\begin{itemize}
    \item Local minima and saddle points
    \item Learning rate selection
    \item Early stopping (intentional regularization)
    \item Batch size effects on gradient noise
\end{itemize}

\subsubsection{Generalization Error}

The gap between training and test performance:
\begin{equation}
    \epsilon_{\text{gen}} = \mathbb{E}_{\text{test}}[\mathcal{L}] - \mathbb{E}_{\text{train}}[\mathcal{L}]
\end{equation}

Controlled by:
\begin{itemize}
    \item Training set size $N$
    \item Model complexity (number of parameters)
    \item Regularization (dropout, weight decay)
    \item Data augmentation
\end{itemize}

%-----------------------------------------------------------------------------
\subsection{PINN-Specific Errors}
%-----------------------------------------------------------------------------

\subsubsection{Collocation Discretization Error}

The continuous physics constraint is approximated by discrete sampling:
\begin{equation}
    \mathcal{L}_{\text{PDE}}^{\text{continuous}} = \int_0^1 \left\| \dv{\vect{y}_\theta}{\zeta} - \vect{f}(\vect{y}_\theta, B) \right\|^2 d\zeta
\end{equation}
\begin{equation}
    \mathcal{L}_{\text{PDE}}^{\text{discrete}} = \frac{1}{K} \sum_{k=1}^{K} \left\| \dv{\vect{y}_\theta}{\zeta}\bigg|_{\zeta_k} - \vect{f}(\vect{y}_\theta(\zeta_k), B(\zeta_k)) \right\|^2
\end{equation}

The discretization error is:
\begin{equation}
    \epsilon_{\text{colloc}} = \left| \mathcal{L}_{\text{PDE}}^{\text{continuous}} - \mathcal{L}_{\text{PDE}}^{\text{discrete}} \right|
\end{equation}

For $K$ uniformly distributed points, by standard quadrature theory:
\begin{equation}
    \epsilon_{\text{colloc}} = \mathcal{O}(K^{-1}) \quad \text{(random sampling)}
\end{equation}

\textbf{Validation procedure:} After training with $K$ points, evaluate the residual on a much finer grid ($K' \gg K$):
\begin{equation}
    R(\zeta) = \left\| \dv{\vect{y}_\theta}{\zeta} - \vect{f}(\vect{y}_\theta, B) \right\|
\end{equation}
If $\max_\zeta R(\zeta) \gg \frac{1}{K}\sum_k R(\zeta_k)$, the network is ``cheating'' between collocation points.

\subsubsection{Inter-Point Oscillation Error}

Between collocation points, the network is unconstrained and may develop spurious oscillations:
\begin{equation}
    \vect{y}_\theta(\zeta) = \vect{y}_{\text{physical}}(\zeta) + \delta\vect{y}(\zeta)
\end{equation}
where $\delta\vect{y}(\zeta_k) \approx 0$ at collocation points but $\delta\vect{y}(\zeta) \neq 0$ between them.

\textbf{Mitigation strategies:}
\begin{enumerate}
    \item Increase $K$ (more collocation points)
    \item Use smooth activation functions (SiLU, Tanh vs ReLU)
    \item Add higher-order derivative penalties
    \item Randomize collocation points each batch (prevents overfitting to fixed $\zeta$)
\end{enumerate}

\subsubsection{Loss Balancing Error}

The total PINN loss combines multiple terms:
\begin{equation}
    \mathcal{L}_{\text{total}} = \Ldata + \lambda_{\text{PDE}} \Lpde + \lambda_{\text{IC}} \Lic
\end{equation}

Poor choice of $\lambda$ values leads to:
\begin{itemize}
    \item $\lambda_{\text{PDE}} \ll 1$: Physics constraints ignored, network behaves like MLP
    \item $\lambda_{\text{PDE}} \gg 1$: Data fit sacrificed, poor endpoint accuracy
\end{itemize}

The optimal $\lambda$ depends on the relative scales and noise levels:
\begin{equation}
    \lambda_{\text{PDE}}^{\text{optimal}} \sim \frac{\text{Var}(\Ldata)}{\text{Var}(\Lpde)}
\end{equation}

\textbf{Adaptive strategies:}
\begin{itemize}
    \item GradNorm: Balance gradient magnitudes
    \item Learning rate annealing per loss term
    \item Curriculum learning: Start with data loss, gradually add physics
\end{itemize}

\subsubsection{Automatic Differentiation Precision}

Computing $\pdv{\vect{y}_\theta}{\zeta}$ via backpropagation introduces floating-point errors:
\begin{equation}
    \left( \pdv{\vect{y}}{\zeta} \right)_{\text{computed}} = \left( \pdv{\vect{y}}{\zeta} \right)_{\text{true}} + \epsilon_{\text{AD}}
\end{equation}

For well-conditioned networks, $\epsilon_{\text{AD}} \sim 10^{-7}$ (float32) or $10^{-15}$ (float64). This is typically negligible compared to other errors.

%-----------------------------------------------------------------------------
\subsection{RK-PINN Specific Errors}
%-----------------------------------------------------------------------------

\subsubsection{Butcher Tableau Approximation}

The RK-PINN uses fixed RK4 coefficients. If the learned force $\hat{\vect{f}}_\theta$ differs from the true force:
\begin{equation}
    \hat{\vect{f}}_\theta(\vect{y}, z) = \vect{f}_{\text{true}}(\vect{y}, z) + \delta\vect{f}(\vect{y}, z)
\end{equation}

The RK4 integration amplifies this error over the step:
\begin{equation}
    \delta \vect{y}_{\text{step}} \approx \frac{h}{6}(\delta\vect{f}_1 + 2\delta\vect{f}_2 + 2\delta\vect{f}_3 + \delta\vect{f}_4)
\end{equation}

\subsubsection{Single-Step vs Multi-Step Error}

The RK-PINN typically takes one large step ($h = \Delta z$) rather than many small steps. The truncation error is:
\begin{equation}
    \epsilon_{\text{RK-PINN}} = \mathcal{O}(h^5) \quad \text{vs} \quad \epsilon_{\text{RK4}} = \mathcal{O}((h/n)^4 \cdot n) = \mathcal{O}(h^4/n^3)
\end{equation}

For the same total $\Delta z$, multi-step RK4 is more accurate. The RK-PINN trades accuracy for speed by learning to correct the single-step error.

%-----------------------------------------------------------------------------
\subsection{Inference-Time Errors}
%-----------------------------------------------------------------------------

\subsubsection{Extrapolation Beyond Training Domain}

If the input $\vect{x}$ falls outside the training distribution:
\begin{equation}
    \vect{x} \notin \text{support}(P_{\text{train}})
\end{equation}

Neural networks can produce arbitrarily wrong predictions. Critical boundaries:
\begin{itemize}
    \item Momentum: $p < p_{\min}^{\text{train}}$ or $p > p_{\max}^{\text{train}}$
    \item Angles: $|t_x|, |t_y| > $ training range
    \item Position: $(x, y)$ outside training envelope
\end{itemize}

\textbf{Mitigation:} Runtime bounds checking, uncertainty estimation, or ensemble disagreement detection.

\subsubsection{Numerical Precision at Inference}

Production deployment may use reduced precision:
\begin{equation}
    \vect{y}_{\text{float16}} = \vect{y}_{\text{float32}} + \epsilon_{\text{quant}}
\end{equation}

For typical track parameters, float16 error is $\sim 10^{-3}$ relative, which may be acceptable depending on requirements.

%-----------------------------------------------------------------------------
\subsection{Error Budget Summary}
%-----------------------------------------------------------------------------

\begin{table}[ht]
\centering
\begin{tabular}{llll}
\toprule
\textbf{Error Source} & \textbf{Typical Magnitude} & \textbf{Control Method} \\
\midrule
\multicolumn{3}{l}{\textit{Training Data Errors}} \\
RK4 ground truth & $10^{-8}$ (normalized) & Decrease step size \\
Field interpolation & $10^{-5}$ T & Use finer grid or cubic splines \\
Gaussian field approx. & $10^{-2}$ T (1.3\%) & Use interpolated field map \\
Sampling bias & Variable & Match physics distributions \\
\midrule
\multicolumn{3}{l}{\textit{Model Errors}} \\
Network capacity & $10^{-4}$--$10^{-3}$ & Increase depth/width \\
Optimization & $10^{-5}$--$10^{-4}$ & Longer training, tuning \\
Generalization & $10^{-4}$--$10^{-3}$ & More data, regularization \\
\midrule
\multicolumn{3}{l}{\textit{PINN-Specific Errors}} \\
Collocation discretization & $\mathcal{O}(K^{-1})$ & Increase $K$ \\
Inter-point oscillation & Network-dependent & Smooth activations, more $K$ \\
Loss balancing ($\lambda$) & Variable & Adaptive $\lambda$, tuning \\
Field gradient discontinuity & $\sim 10^{-5}$ & Cubic spline interpolation \\
\midrule
\multicolumn{3}{l}{\textit{Inference-Time Errors}} \\
Domain extrapolation & Unbounded & Input validation \\
Numerical precision (fp16) & $\sim 10^{-3}$ relative & Use fp32 if needed \\
\bottomrule
\end{tabular}
\caption{Comprehensive summary of error sources and mitigation strategies.}
\label{tab:error_budget}
\end{table}

%-----------------------------------------------------------------------------
\subsection{Recommended Validation Protocol}
%-----------------------------------------------------------------------------

To ensure robust deployment, we recommend the following validation checks:

\begin{enumerate}
    \item \textbf{Convergence check}: Verify loss curves have plateaued; early stopping should trigger.
    
    \item \textbf{Train/validation/test split}: Report metrics on held-out test set, not training data.
    
    \item \textbf{Physics residual analysis} (PINN): Evaluate $R(\zeta)$ on fine grid; ensure no inter-point violations.
    
    \item \textbf{Momentum-binned metrics}: Report accuracy separately for low-$p$, mid-$p$, high-$p$ tracks.
    
    \item \textbf{Comparison to RK4}: On test set, compare against high-precision RK4 integration.
    
    \item \textbf{Trajectory validation}: For PINN, compare predicted intermediate states against RK4 trajectories.
    
    \item \textbf{Edge case testing}: Evaluate on extreme angles, minimum/maximum momentum, boundary positions.
    
    \item \textbf{Charge symmetry}: Verify equal performance for $q > 0$ and $q < 0$ tracks.
    
    \item \textbf{Reproducibility}: Train multiple times with different seeds; report mean $\pm$ std of metrics.
\end{enumerate}

%=============================================================================
\section{Conclusion}
%=============================================================================

We have derived the complete mathematical framework for neural network-based track extrapolation:

\begin{enumerate}
    \item \textbf{MLP}: Learns the extrapolation mapping implicitly from data
    \item \textbf{PINN}: Enforces Lorentz force equations via automatic differentiation
    \item \textbf{RK-PINN}: Combines RK structure with learnable physics constraints
\end{enumerate}

The key physics are encoded in Equations~\eqref{eq:dtxdz}--\eqref{eq:dtydz}, with the critical constant $\clight = 2.99792458 \times 10^{-4}$.

A comprehensive error analysis (Section~\ref{tab:error_budget}) identifies the dominant error sources: training data quality, collocation discretization for PINNs, and domain extrapolation at inference. Rigorous validation protocols are essential before deployment in production.

\end{document}

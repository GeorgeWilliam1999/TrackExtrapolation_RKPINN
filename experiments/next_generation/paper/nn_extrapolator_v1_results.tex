%===============================================================================
% Neural Network Track Extrapolator - V1 Analysis Report
%
% Author: G. Scriven
% Date: January 2026
% LHCb Track Extrapolation Project
%===============================================================================

\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{float}
\usepackage{subcaption}
\usepackage{listings}

% Simple macros to replace physics package
\newcommand{\vb}[1]{\mathbf{#1}}
\newcommand{\dd}[1]{\mathrm{d}#1}
\newcommand{\dv}[2]{\frac{\mathrm{d}#1}{\mathrm{d}#2}}
\newcommand{\pdv}[2]{\frac{\partial #1}{\partial #2}}

% No additional SI unit macros needed

\geometry{margin=2.5cm}

% Custom colors
\definecolor{mlpcolor}{RGB}{52, 152, 219}
\definecolor{pinncolor}{RGB}{155, 89, 182}
\definecolor{rkpinncolor}{RGB}{231, 76, 60}

% Title
\title{\textbf{Neural Network Track Extrapolator}\\
\Large V1 Training Analysis Report}
\author{G. Scriven\\
LHCb Collaboration\\
NIKHEF, Amsterdam}
\date{January 2026}

\begin{document}

\maketitle

%===============================================================================
\begin{abstract}
We present the results of V1 training for neural network-based track extrapolation in the LHCb detector. Three architecture types were explored: Multi-Layer Perceptron (MLP), Physics-Informed Neural Network (PINN), and Runge-Kutta Physics-Informed Neural Network (RK\_PINN). A total of 53 model configurations were trained on 50 million track extrapolation samples. Our findings show that small MLP architectures achieve inference times competitive with existing C++ extrapolators (1.1--2.2~$\mu$s vs 1.5--2.5~$\mu$s) while maintaining sub-millimeter position accuracy. Contrary to expectations, physics-informed approaches did not improve accuracy over pure data-driven MLPs, while significantly increasing computational cost.
\end{abstract}

%===============================================================================
\section{Introduction}
\label{sec:intro}

Track extrapolation is a fundamental operation in particle physics reconstruction, required for pattern recognition, track fitting, and physics analysis. In LHCb, charged particles traverse the detector's magnetic field, following curved trajectories governed by the Lorentz force. The standard approach uses numerical integration (Runge-Kutta methods) of the equations of motion, which is accurate but computationally intensive.

This study investigates replacing the C++ Cash-Karp Runge-Kutta extrapolator with neural networks trained on simulated trajectory data. The potential benefits include:
\begin{itemize}
    \item \textbf{Speed:} Neural network inference can be parallelised on GPUs and optimised for specific hardware
    \item \textbf{Simplicity:} Single forward pass vs.\ iterative numerical integration
    \item \textbf{Differentiability:} Enables gradient-based optimisation of downstream tasks
\end{itemize}

The V1 training campaign explored three architecture families:
\begin{enumerate}
    \item \textbf{MLP:} Standard feedforward networks trained with supervised learning
    \item \textbf{PINN:} Networks augmented with physics-based loss terms
    \item \textbf{RK\_PINN:} Multi-stage architecture inspired by RK4 numerical integration
\end{enumerate}

%===============================================================================
\section{Mathematical Foundations}
\label{sec:maths}

\subsection{Track Extrapolation Problem}

A charged particle in a magnetic field follows a trajectory governed by the Lorentz force:
\begin{equation}
    \vb{F} = q(\vb{v} \times \vb{B})
\end{equation}
where $q$ is the particle charge, $\vb{v}$ is the velocity, and $\vb{B}$ is the magnetic field.

In LHCb, tracks are parameterised along the beam axis $z$. The state vector at position $z$ is:
\begin{equation}
    \vb{y}(z) = \begin{pmatrix} x \\ y \\ t_x \\ t_y \end{pmatrix}
\end{equation}
where $(x, y)$ is the transverse position and $(t_x, t_y) = (\dd{x}/\dd{z}, \dd{y}/\dd{z})$ are the track slopes.

The equations of motion in $z$-parameterisation are:
\begin{align}
    \dv{x}{z} &= t_x \\
    \dv{y}{z} &= t_y \\
    \dv{t_x}{z} &= \kappa N \left[ t_x t_y B_x - (1 + t_x^2) B_y + t_y B_z \right] \\
    \dv{t_y}{z} &= \kappa N \left[ (1 + t_y^2) B_x - t_x t_y B_y - t_x B_z \right]
\end{align}
where:
\begin{itemize}
    \item $\kappa = (q/p) \cdot c_\text{light}$ with $c_\text{light} = 2.99792458 \times 10^{-4}$ GeV/(T$\cdot$mm)
    \item $N = \sqrt{1 + t_x^2 + t_y^2}$ is the normalisation factor
    \item $q/p$ is the charge-over-momentum in 1/MeV
\end{itemize}

\subsection{Neural Network Formulation}

The extrapolation task is formulated as a regression problem:
\begin{equation}
    f_\theta : \mathbb{R}^6 \to \mathbb{R}^4
\end{equation}
\begin{equation}
    \vb{x} = (x_0, y_0, t_{x,0}, t_{y,0}, q/p, \Delta z) \mapsto \vb{y}_f = (x_f, y_f, t_{x,f}, t_{y,f})
\end{equation}
where $\vb{x}$ is the input (initial state + step size) and $\vb{y}_f$ is the predicted final state.

%===============================================================================
\section{Network Architectures}
\label{sec:architectures}

\subsection{MLP (Multi-Layer Perceptron)}

The MLP is a standard feedforward network (Figure~\ref{fig:mlp_arch}):
\begin{equation}
    \vb{y} = \sigma_{\text{out}} \circ W_L \circ \sigma \circ W_{L-1} \circ \cdots \circ \sigma \circ W_1 (\hat{\vb{x}})
\end{equation}
where $\hat{\vb{x}}$ is the normalised input, $W_i$ are linear transformations, and $\sigma$ is the SiLU activation:
\begin{equation}
    \sigma_{\text{SiLU}}(x) = x \cdot \text{sigmoid}(x) = \frac{x}{1 + e^{-x}}
\end{equation}

\textbf{Training loss:} Pure supervised learning
\begin{equation}
    \mathcal{L}_{\text{MLP}} = \frac{1}{N} \sum_{i=1}^{N} \| \hat{\vb{y}}_i - \vb{y}_i^* \|^2
\end{equation}

\begin{figure}[htbp]
    \centering
    \includegraphics[height=0.35\textheight]{../analysis/plots/mlp_architectures.png}
    \caption{MLP architecture configurations explored in V1 training, ranging from Tiny (5k parameters) to Large (400k parameters).}
    \label{fig:mlp_arch}
\end{figure}

\subsection{PINN (Physics-Informed Neural Network)}

The PINN uses the same network structure but augments the loss with physics constraints (Figure~\ref{fig:pinn_flow}):
\begin{equation}
    \mathcal{L}_{\text{PINN}} = \mathcal{L}_{\text{data}} + \lambda_{\text{IC}} \mathcal{L}_{\text{IC}} + \lambda_{\text{PDE}} \mathcal{L}_{\text{PDE}}
\end{equation}

\textbf{Initial Condition Loss:} Ensures the network prediction at $z=0$ matches the input state:
\begin{equation}
    \mathcal{L}_{\text{IC}} = \| f_\theta(\vb{x}_0, z=0) - \vb{x}_0 \|^2
\end{equation}

\textbf{PDE Residual Loss:} Enforces the Lorentz force equations at collocation points $z_i$:
\begin{equation}
    \mathcal{L}_{\text{PDE}} = \sum_{i=1}^{N_c} \left\| \pdv{\hat{\vb{y}}_i}{z} - \vb{F}(\hat{\vb{y}}_i, \vb{B}(z_i)) \right\|^2
\end{equation}
where the derivatives are computed via automatic differentiation.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../analysis/plots/pinn_training_flow.png}
    \caption{PINN training flow showing the three loss components: data loss at endpoint, IC loss at $z=0$, and PDE residual at collocation points.}
    \label{fig:pinn_flow}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../analysis/plots/pinn_trajectory_loss.png}
    \caption{Visualisation of PINN loss components along a particle trajectory. Green: initial condition check. Purple: PDE residual at collocation points. Yellow: data loss at endpoint.}
    \label{fig:pinn_traj}
\end{figure}

\subsection{RK\_PINN (Runge-Kutta PINN)}

The RK\_PINN architecture is inspired by the classical RK4 integration scheme (Figure~\ref{fig:rkpinn_arch}):

\textbf{Classical RK4:}
\begin{align}
    k_1 &= f(t_n, y_n) \\
    k_2 &= f(t_n + h/2, y_n + h k_1/2) \\
    k_3 &= f(t_n + h/2, y_n + h k_2/2) \\
    k_4 &= f(t_n + h, y_n + h k_3) \\
    y_{n+1} &= y_n + \frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)
\end{align}

\textbf{RK\_PINN Network:}
\begin{itemize}
    \item Shared backbone extracts features from initial state
    \item Four stage heads predict state at $z = \{0.25, 0.5, 0.75, 1.0\} \cdot \Delta z$
    \item Learnable combination weights (initialised to RK4: $[1, 2, 2, 1]/6$)
\end{itemize}
\begin{equation}
    \hat{\vb{y}}_{\text{final}} = \sum_{k=1}^{4} w_k \cdot \hat{\vb{y}}_k, \quad w_k = \text{softmax}(\tilde{w}_k)
\end{equation}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../analysis/plots/rk_pinn_architecture.png}
    \caption{RK\_PINN multi-stage architecture with shared backbone and four stage heads at fractional z positions.}
    \label{fig:rkpinn_arch}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../analysis/plots/rk_pinn_trajectory_loss.png}
    \caption{RK\_PINN 4-stage predictions along a trajectory. Each stage contributes to the final prediction with learnable weights initialised to RK4 values.}
    \label{fig:rkpinn_traj}
\end{figure}

\subsection{Architecture Comparison}

Figure~\ref{fig:arch_comparison} summarises the key differences between the three architecture types.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../analysis/plots/architecture_comparison.png}
    \caption{Comparison of MLP, PINN, and RK\_PINN architectures showing network structure, loss functions, and characteristics.}
    \label{fig:arch_comparison}
\end{figure}

%===============================================================================
\section{Training Configuration}
\label{sec:training}

\subsection{Dataset}

Training data was generated using a C++ RK4 integrator with the LHCb magnetic field map:
\begin{itemize}
    \item \textbf{Samples:} 50 million track extrapolations
    \item \textbf{Split:} 80\% train, 10\% validation, 10\% test
    \item \textbf{Field model:} Interpolated from \texttt{twodip.rtf} field map
    \item \textbf{z range:} 0--9500 mm (full detector acceptance)
    \item \textbf{Momentum:} 1--100 GeV uniformly sampled
\end{itemize}

\subsection{Training Parameters}

\begin{table}[htbp]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Optimiser & AdamW \\
Learning rate & $10^{-3}$ \\
Weight decay & $10^{-4}$ \\
Batch size & 8192 \\
Epochs & 10 \\
Scheduler & Cosine annealing \\
Warmup epochs & 5 \\
Gradient clipping & 1.0 \\
\bottomrule
\end{tabular}
\caption{Training hyperparameters used for V1 models.}
\end{table}

\subsection{Model Configurations}

A total of 53 models were trained across the three architecture types:

\begin{table}[htbp]
\centering
\begin{tabular}{llccc}
\toprule
\textbf{Type} & \textbf{Hidden Dims} & \textbf{Params} & \textbf{Best Val Loss} & \textbf{Count} \\
\midrule
MLP & [64, 64] (tiny) & 5k & 0.0031 & 2 \\
MLP & [128, 128] (small) & 18k & 0.0013 & 3 \\
MLP & [256, 256, 128] (medium) & 100k & 0.0006 & 6 \\
MLP & [512, 512, 256] (large) & 399k & 0.0004 & 3 \\
MLP & [512, 512, 256, 128] (wide) & 431k & 0.0010 & 3 \\
\midrule
PINN & [256, 256, 128] (medium) & 100k & 0.0036 & 8 \\
PINN & [512, 512, 256] (large) & 399k & 0.0042 & 4 \\
\midrule
RK\_PINN & [256, 256, 128] (medium) & 202k & 0.0039 & 12 \\
RK\_PINN & [512, 512, 256] (large) & 797k & 0.0042 & 4 \\
\bottomrule
\end{tabular}
\caption{V1 model configurations and best validation loss achieved.}
\end{table}

%===============================================================================
\section{Results}
\label{sec:results}

\subsection{Accuracy Comparison}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{../analysis/plots/parameter_comparison.png}
    \caption{Parameter counts across architecture presets. Note that RK\_PINN has approximately 2$\times$ more parameters than MLP/PINN for the same hidden dimensions due to the 4 stage heads.}
    \label{fig:param_comparison}
\end{figure}

Table~\ref{tab:top_models} shows the best performing models from V1 training:

\begin{table}[htbp]
\centering
\begin{tabular}{lllcc}
\toprule
\textbf{Rank} & \textbf{Model} & \textbf{Type} & \textbf{Params} & \textbf{Val Loss} \\
\midrule
1 & mlp\_large\_v1 & MLP & 399k & 0.00044 \\
2 & mlp\_medium & MLP & 100k & 0.00058 \\
3 & rkpinn\_medium\_data\_only & RK\_PINN & 100k & 0.00067 \\
4 & mlp\_wide & MLP & 431k & 0.00095 \\
5 & mlp\_medium\_v1 & MLP & 100k & 0.00102 \\
6 & mlp\_wide\_v1 & MLP & 431k & 0.00117 \\
7 & rkpinn\_medium\_pde\_weak & RK\_PINN & 100k & 0.00131 \\
8 & mlp\_small & MLP & 18k & 0.00134 \\
9 & mlp\_balanced\_v1 & MLP & 57k & 0.00209 \\
10 & rkpinn\_wide & RK\_PINN & 431k & 0.00251 \\
\bottomrule
\end{tabular}
\caption{Top 10 V1 models ranked by validation loss.}
\label{tab:top_models}
\end{table}

\textbf{Key observation:} The top performing models are all MLPs or RK\_PINNs with physics loss disabled ($\lambda_{\text{PDE}} = 0$). Pure PINN models with physics constraints perform worse.

\subsection{Timing Performance}

Inference timing was benchmarked for all neural network models and compared against the existing C++ extrapolators in LHCb. The benchmarks were performed on CPU.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../analysis/plots/timing_benchmarks.png}
    \caption{Left: Neural network inference times by model. Right: Direct comparison between NN models and C++ extrapolators. Small MLPs achieve comparable timing to C++ implementations.}
    \label{fig:timing}
\end{figure}

\begin{table}[htbp]
\centering
\small
\begin{tabular}{llccc}
\toprule
\textbf{Type} & \textbf{Model} & \textbf{Params} & \textbf{Time ($\mu$s)} & \textbf{Notes} \\
\midrule
\multicolumn{5}{l}{\textit{C++ Extrapolators}} \\
C++ & Kisel & -- & 1.50 & Fast, 39.8 mm error \\
C++ & Herab & -- & 1.95 & 0.76 mm error \\
C++ & BogackiShampine3 & -- & 2.40 & 0.10 mm error \\
C++ & Reference (RK4) & -- & 2.50 & Ground truth \\
C++ & Verner9 & -- & 2.52 & High-order \\
\midrule
\multicolumn{5}{l}{\textit{Neural Network Models}} \\
MLP & mlp\_tiny & 4.9k & 1.12 & Fastest NN \\
PINN & pinn\_tiny & 4.9k & 1.19 & \\
MLP & mlp\_small & 17.9k & 1.22 & \\
PINN & pinn\_small & 17.9k & 1.31 & \\
MLP & mlp\_medium & 101k & 2.17 & Best accuracy/speed \\
PINN & pinn\_medium & 101k & 2.46 & \\
RK\_PINN & rkpinn\_tiny & 18.4k & 3.91 & RK overhead \\
RK\_PINN & rkpinn\_small & 69.5k & 4.06 & \\
PINN & pinn\_wide & 431k & 4.62 & \\
MLP & mlp\_wide & 431k & 4.70 & \\
RK\_PINN & rkpinn\_medium & 202k & 5.59 & \\
RK\_PINN & rkpinn\_wide & 532k & 7.83 & Slowest NN \\
\bottomrule
\end{tabular}
\caption{Timing comparison: C++ extrapolators vs neural network models (CPU inference).}
\label{tab:timing}
\end{table}

\textbf{Key findings:}
\begin{itemize}
    \item \textbf{Small MLPs are competitive:} mlp\_tiny (1.12~$\mu$s) and mlp\_small (1.22~$\mu$s) are faster than most C++ extrapolators
    \item \textbf{Medium models match C++:} mlp\_medium (2.17~$\mu$s) is comparable to the Reference RK4 (2.50~$\mu$s)
    \item \textbf{RK\_PINNs have overhead:} The multi-head architecture adds 2--4$\times$ overhead vs equivalent MLPs
    \item \textbf{Large models are slower:} Wide models (4.6--7.8~$\mu$s) are slower than C++ implementations
    \item \textbf{Trade-off:} mlp\_medium offers the best accuracy (val loss 0.0006) at comparable speed to C++
\end{itemize}

\subsection{Convergence Analysis}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../analysis/plots/loss_curves_architecture.png}
    \caption{Training and validation loss curves for different architectures.}
    \label{fig:loss_curves}
\end{figure}

%===============================================================================
\section{Error Analysis}
\label{sec:errors}

\subsection{Position Errors}

The position error (residual) is defined as:
\begin{equation}
    \varepsilon_{\text{pos}} = \sqrt{(x_{\text{pred}} - x_{\text{true}})^2 + (y_{\text{pred}} - y_{\text{true}})^2}
\end{equation}

For the best MLP model (mlp\_large\_v1):
\begin{itemize}
    \item Mean position error: 0.3 mm
    \item 95th percentile: 0.8 mm
    \item Maximum: 2.1 mm (outliers at very low momentum)
\end{itemize}

\subsection{Systematic Biases}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{../analysis/plots/momentum_comparison.png}
    \caption{Error distribution as a function of particle momentum. Low momentum tracks show larger errors due to stronger bending.}
    \label{fig:momentum}
\end{figure}

Analysis revealed several systematic effects:

\begin{enumerate}
    \item \textbf{Momentum dependence:} Errors increase at low momentum ($p < 5$ GeV) where curvature is largest
    \item \textbf{Step size dependence:} Large extrapolation steps ($\Delta z > 5000$ mm) accumulate more error
    \item \textbf{Field region:} Errors peak in the dipole field region (4000--6000 mm)
\end{enumerate}

\subsection{PINN Training Failures}

Several PINN models exhibited training instabilities:

\begin{itemize}
    \item \textbf{NaN losses:} 15\% of PINN runs produced NaN gradients
    \item \textbf{Cause:} Physics loss scales as $\mathcal{O}(10^{26})$ when predictions are far from initialisation
    \item \textbf{Mitigation:} Reduced physics loss weight ($\lambda_{\text{PDE}} < 0.01$) or disabled entirely
\end{itemize}

\subsection{Why Physics-Informed Methods Underperformed}

Several hypotheses explain the poor performance of PINNs:

\begin{enumerate}
    \item \textbf{Data abundance:} With 50M samples, pure data-driven learning is sufficient; physics constraints become redundant
    \item \textbf{Field model mismatch:} PINN uses Gaussian field approximation; data generated with interpolated field map
    \item \textbf{Optimisation difficulty:} Multi-objective loss with competing gradients from data and physics terms
    \item \textbf{Computational overhead:} Automatic differentiation for physics loss is expensive
\end{enumerate}

%===============================================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{MLP is optimal:} Simple feedforward networks achieve the best accuracy/speed trade-off
    \item \textbf{Physics constraints hurt:} PINN and RK\_PINN are slower and less accurate than MLPs
    \item \textbf{Small models suffice:} Even the tiny MLP (5k params) achieves 1.12~$\mu$s inference, faster than most C++ extrapolators
    \item \textbf{Diminishing returns:} Beyond 100k parameters, accuracy improvements are marginal
\end{enumerate}

%===============================================================================
\section{Conclusion}
\label{sec:conclusion}

V1 training successfully demonstrated that neural networks can achieve competitive performance with existing C++ extrapolators. The key finding is that simple MLP architectures outperform physics-informed approaches both in accuracy and inference speed.

The best model (mlp\_large\_v1) achieves:
\begin{itemize}
    \item Validation loss: 0.00044
    \item Position error: $< 0.5$ mm (mean)
    \item Inference time: comparable to C++ (mlp\_medium: 2.17~$\mu$s vs Reference RK4: 2.50~$\mu$s)
\end{itemize}

For production deployment, we recommend the mlp\_small model as the optimal balance:
\begin{itemize}
    \item Only 18k parameters (35 KB ONNX file)
    \item Inference time: 1.22~$\mu$s (faster than C++ Reference at 2.50~$\mu$s)
    \item Good accuracy (val loss 0.0013) for most use cases
\end{itemize}

V2 training will explore shallow-wide architectures and extended training to further optimise performance.

%===============================================================================
\appendix

\section{V1 Model Summary}
\label{app:models}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{../analysis/plots/v1_models_table.png}
    \caption{Complete list of V1 trained models with configurations.}
    \label{fig:v1_table}
\end{figure}

%===============================================================================
\end{document}

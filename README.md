# TrackExtrapolators - Next Generation ML Models

**Clean repository for next-generation track extrapolation experiments**

Reorganized: January 14, 2025  
Previous work archived in: `legacy/`

---

## ğŸ¯ Project Status

**Current Phase:** âœ… **C++ Baselines Established**  
**Goal:** Train next-generation ML track extrapolators for LHCb

**Completed:**
- âœ… LHCb software stack properly configured (DetDesc mode)
- âœ… C++ extrapolator tests running successfully  
- âœ… All 9 extrapolators benchmarked across 1210 track states
- âœ… Quantitative accuracy analysis complete
  - **Best:** BogackiShampine3, Verner9 (0.10mm mean error)
  - **Fast:** Herab (0.76mm mean error)
  - **Problematic:** Kisel (39.8mm mean error)

**Next Steps:**
1. Generate training data with validated RK4 parameters
2. Train baseline MLP to beat Herab's 0.76mm accuracy
3. Implement timing benchmarks for ML inference
4. Compare ML performance vs C++ baselines

---

## ğŸ“‚ Repository Structure

```
TrackExtrapolators/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ CMakeLists.txt                     # C++ build system
â”‚
â”œâ”€â”€ src/                               # C++ Production Code (LHCb framework)
â”‚   â”œâ”€â”€ TrackRungeKuttaExtrapolator.cpp    # RK4 baseline (to benchmark)
â”‚   â”œâ”€â”€ TrackKiselExtrapolator.cpp         # Fast analytic method
â”‚   â”œâ”€â”€ TrackSTEPExtrapolator.cpp          # Reference (highest accuracy)
â”‚   â”œâ”€â”€ ExtrapolatorTester.cpp             # Simple benchmark tool
â”‚   â””â”€â”€ TrackExtrapolatorTesterSOA.cpp     # Full benchmark with timing
â”‚
â”œâ”€â”€ tests/                             # LHCb framework tests
â”‚   â”œâ”€â”€ options/                       # Gaudi configuration files
â”‚   â””â”€â”€ qmtest/                        # LHCb test descriptors
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ next_generation/               # ğŸ†• ACTIVE DEVELOPMENT
â”‚   â”‚   â”œâ”€â”€ EXPERIMENT_DESIGN.md       # Full experimental plan
â”‚   â”‚   â”œâ”€â”€ REFERENCES.md              # Literature review
â”‚   â”‚   â”œâ”€â”€ DATA_AND_MODEL_MANAGEMENT.md   # Infrastructure design
â”‚   â”‚   â”œâ”€â”€ REVIEW_AND_GAPS.md         # Gap analysis (review)
â”‚   â”‚   â”œâ”€â”€ GAP_ANALYSIS_FINDINGS.md   # Investigation results
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ benchmarking/              # C++ baseline benchmarks
â”‚   â”‚   â”‚   â”œâ”€â”€ README.md              # How to benchmark
â”‚   â”‚   â”‚   â””â”€â”€ benchmark_cpp.py       # Python wrapper
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ data_generation/           # Training data creation
â”‚   â”‚   â”‚   â”œâ”€â”€ generate_data.py       # Main script
â”‚   â”‚   â”‚   â””â”€â”€ datasets/              # Generated .npy files
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ training/                  # Model training scripts
â”‚   â”‚   â”œâ”€â”€ analysis/                  # Result analysis notebooks
â”‚   â”‚   â”œâ”€â”€ deployment/                # ONNX export, C++ integration
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ utils/                     # Shared utilities
â”‚   â”‚       â””â”€â”€ rk4_propagator.py      # Pure Python RK4 integrator
â”‚   â”‚
â”‚   â”œâ”€â”€ experiment_log.csv             # Experiment tracking
â”‚   â””â”€â”€ README.md                      # Experiment guidelines
â”‚
â””â”€â”€ legacy/                            # ğŸ“¦ ARCHIVED (previous work)
    â”œâ”€â”€ old_notebooks/                 # Analysis notebooks
    â”œâ”€â”€ old_experiments/               # All previous experiments
    â”œâ”€â”€ old_python_scripts/ml_models/  # ML training code & trained models
    â”œâ”€â”€ plots/                         # Old plots
    â”œâ”€â”€ report/                        # LaTeX report
    â”œâ”€â”€ lhcb-metainfo/                 # Metadata
    â””â”€â”€ OLD_README.md                  # Previous README (0.21mm claims)
```

---

## ğŸš€ Quick Start

### Prerequisites

This project uses the LHCb software stack. You must have:
- Access to CVMFS (e.g., Nikhef STBC cluster)
- LHCb stack built with DetDesc geometry backend
- Environment: `x86_64_v2-el9-gcc13+detdesc-opt`

**Setup location:** `/data/bfys/gscriven/TE_stack/`

### 1. Run Existing C++ Tests

Verify the LHCb framework is working correctly:

```bash
# From the stack directory
cd /data/bfys/gscriven/TE_stack

# Run the standard extrapolator test
Rec/run gaudirun.py Rec/Tr/TrackExtrapolators/tests/qmtest/test_extrapolators.qmt

# Should see: All extrapolators running successfully with accuracy comparisons
```

This tests 9 different extrapolators across a grid of track states.

### 2. Run Comprehensive Benchmarks

```bash
# Run the benchmark configuration
cd /data/bfys/gscriven/TE_stack
Rec/run gaudirun.py Rec/Tr/TrackExtrapolators/tests/options/benchmark_extrapolators.py

# Check timing output in the log
# Look for "Timing table" showing execution time per algorithm
```

**Benchmark includes:**
- Reference methods: RK4 (multiple schemes)
- Fast approximations: Kisel, Herab, Linear, Parabolic
- Accuracy: Compared against high-precision STEP integrator
- Test grid: 11Ã—11 = 121 track states per extrapolator

### 3. Extract Performance Metrics

```bash
cd /data/bfys/gscriven/TE_stack/Rec/Tr/TrackExtrapolators/experiments/next_generation/benchmarking

# Parse the benchmark log (when available)
# python parse_benchmark_results.py

# Analyze results
# jupyter notebook analyze_benchmarks.ipynb
```

### 4. Generate Training Data (Future)

```bash
cd experiments/next_generation/data_generation

# Generate training set (10K tracks, 5mm step size)
python generate_data.py --n-tracks 10000 --name train --step-size 5.0

# Generate validation set
python generate_data.py --n-tracks 2000 --name val --step-size 5.0

# Generate test set
python generate_data.py --n-tracks 2000 --name test --step-size 5.0
```

**Output:** `datasets/X_train.npy`, `Y_train.npy`, `P_train.npy` (and val, test)

### 3. Train Model (TODO - Coming Next)

```bash
cd experiments/next_generation/training

# Will create training scripts next
# python train_mlp.py --architecture small
```

---

## ğŸ“Š Legacy Results (For Reference)

Previous experiments (now in `legacy/`) achieved:

| Model | Activation | Mean Error | Dataset | Notes |
|-------|------------|------------|---------|-------|
| MLP (SiLU) | SiLU | **0.334 mm** | 50K tracks | Best from legacy |
| MLP (Tanh) | Tanh | 0.63 mm | 50K tracks | Baseline |
| PINN | Various | 18-329 mm | 50K tracks | âŒ Failed |

**Architecture:** [128, 128, 64] with 25,924 parameters  
**Data source:** Python RK4 with analytical field (Bâ‚€=1.0T)

---

## ğŸ”¬ Active Development: Next Generation

### Current Tasks

- [x] **Setup LHCb environment** - Stack built with DetDesc backend
- [x] **Verify C++ tests** - All extrapolators working correctly
- [x] **Run comprehensive benchmarks** - 9 extrapolators tested
- [ ] **Extract timing metrics** - Parse logs for performance data
- [ ] **Analyze accuracy** - Quantify errors vs reference method
- [ ] Generate pilot dataset (10K tracks) with validated step size
- [ ] Train baseline MLP and compare to legacy (0.334mm claim)
- [ ] Implement uncertainty quantification

### Design Documents

Read in this order:
1. [EXPERIMENT_DESIGN.md](experiments/next_generation/EXPERIMENT_DESIGN.md) - Full plan
2. [GAP_ANALYSIS_FINDINGS.md](experiments/next_generation/GAP_ANALYSIS_FINDINGS.md) - What exists vs what's needed
3. [DATA_AND_MODEL_MANAGEMENT.md](experiments/next_generation/DATA_AND_MODEL_MANAGEMENT.md) - Infrastructure

### Recent Breakthroughs

**LHCb Software Configuration (Jan 14, 2025):**
- âœ… Tests must be run via `Rec/run` script (not direct `gaudirun.py`)
- âœ… Conditions database requires proper PyConf setup with `testfiledb`
- âœ… SSH authentication to CERN GitLab working (port 7999)
- âœ… CVMFS resources accessible (field maps, detector DB, lhcb-metainfo)

**Benchmark Results:**
- All 9 extrapolators running successfully
- Test grid: 11Ã—11 = 121 track states (various momenta and angles)
- Total execution: ~0.286s for full benchmark suite
- Methods tested: Reference RK4, BogackiShampine3, Verner7/9, Tsitouras5, Kisel, Herab, Linear, Parabolic

---

## ğŸ“‹ Key Learnings

### LHCb Software Stack

**Correct way to run tests:**
```bash
# From stack directory (/data/bfys/gscriven/TE_stack)
Rec/run gaudirun.py <path-to-options-file>

# NOT: gaudirun.py <path> (missing environment setup)
```

**Test files:**
- `.qmt` files: QMTest descriptors (reference expected output)
- `.py` files in `tests/options/`: Gaudi configuration scripts
- `.ref` files in `tests/refs/`: Expected output for validation

**Adding new extrapolators** (from supervisor guide):
1. Copy existing extrapolator (e.g., `TrackKiselExtrapolator.cpp`)
2. Rename class and update CMakeLists.txt
3. Implement `propagate()` method (line ~68 in template)
4. Key function signature:
   ```cpp
   StatusCode propagate(
       Gaudi::TrackVector& stateVec,  // [x, y, tx, ty, q/p]
       double zOld, double zNew,
       Gaudi::TrackMatrix* transMat,  // Transport matrix (optional)
       IGeometryInfo const& geometry,
       LHCb::Tr::PID pid,
       const LHCb::Magnet::MagneticFieldGrid* grid
   ) const override;
   ```

**Simplest reference:** `TrackLinearExtrapolator.cpp` - straight-line propagation

---

## ğŸ› ï¸ Dependencies

### C++ (LHCb Framework)
- Gaudi
- LHCb software stack
- Eigen3 (for ML inference)
- ROOT (for benchmarking)

### Python
```bash
pip install numpy torch tensorboard scikit-learn
```

Optional for benchmarking:
```bash
pip install uproot awkward  # For parsing ROOT files without PyROOT
```

---

## ğŸ“ Experiment Tracking

All experiments logged in [experiments/experiment_log.csv](experiments/experiment_log.csv)

---

## ğŸ¤ Workflow

1. Work in `experiments/next_generation/`
2. Log experiments to `experiment_log.csv`
3. Save models with metadata JSON
4. Update relevant README when completing milestones

---

## âš ï¸ Important Notes

### From User

> "I THINK it is 5mm" - RK4 step size (need to verify by benchmarking)

> "This is acceptable for now but when we have the true map we will need to re run everything" - Regarding analytical field model

### Critical Next Step

**BENCHMARK THE C++ RK4 FIRST!** 

Without baseline timing, we can't validate the "10Ã— speedup" target.

Expected: ~50-150 Î¼s per track  
Target ML: < 15 Î¼s per track (10Ã— faster)

---

**Last Updated:** January 14, 2025  
**Status:** Repository reorganized, ready for next-generation experiments

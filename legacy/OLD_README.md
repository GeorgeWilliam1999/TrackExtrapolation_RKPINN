# LHCb Track Extrapolators

Neural network-based track extrapolation for the LHCb experiment, replacing traditional numerical integration methods.

---

## ğŸ¯ Project Overview

**Goal:** Develop fast, accurate ML-based track extrapolators for LHCb reconstruction

**Current Status:** âœ… Proof of concept complete
- **Best Model:** MLP with SiLU activation
- **Performance:** 0.21 mm mean error (vs 0.63 mm tanh, 0.77 mm ReLU)
- **Speed:** ~160Ã— faster than Runge-Kutta (estimated)
- **Physics:** Pure data-driven learning outperforms physics-informed approaches

**Next Steps:** Scale to full detector coverage, optimize for production

---

## ğŸ“‚ Repository Structure

```
TrackExtrapolators/
â”‚
â”œâ”€â”€ README.md                           # This file - project overview
â”œâ”€â”€ model_investigation.ipynb           # ğŸ“Š Main analysis notebook (START HERE)
â”œâ”€â”€ analyze_extrapolators.ipynb         # Legacy analysis
â”œâ”€â”€ extrapolator_results.csv            # Benchmark results
â”œâ”€â”€ full_domain_benchmark.ipynb         # Full coverage analysis
â”‚
â”œâ”€â”€ ml_models/                          # ğŸ§  Machine Learning Pipeline
â”‚   â”œâ”€â”€ README.md                       # ML documentation (usage, training)
â”‚   â”œâ”€â”€ data/                           # Training datasets
â”‚   â”‚   â”œâ”€â”€ X_analysis.npy              # Input: 50K samples (6D states @ z=4000mm)
â”‚   â”‚   â”œâ”€â”€ Y_analysis.npy              # Output: extrapolated states @ z=12000mm
â”‚   â”‚   â”œâ”€â”€ P_analysis.npy              # Momentum for each track
â”‚   â”‚   â””â”€â”€ X_weighted_train.npy        # Weighted training data (legacy)
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                         # Trained models
â”‚   â”‚   â”œâ”€â”€ analysis/                   # Analysis models (latest)
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp_act_silu.pt         # â­ BEST: 0.21mm error
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp_act_tanh.pt         # Baseline: 0.63mm
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp_act_relu.pt         # 0.77mm
â”‚   â”‚   â”‚   â”œâ”€â”€ pinn_lambda_0_01.pt     # Physics-informed: 18.8mm (failed)
â”‚   â”‚   â”‚   â”œâ”€â”€ pinn_lambda_0_05.pt     # 106mm (failed)
â”‚   â”‚   â”‚   â”œâ”€â”€ pinn_lambda_0_1.pt      # 197mm (failed)
â”‚   â”‚   â”‚   â””â”€â”€ pinn_lambda_0_2.pt      # 329mm (failed badly)
â”‚   â”‚   â””â”€â”€ production/                 # Production models (HTCondor training)
â”‚   â”‚       â””â”€â”€ (empty - jobs pending)
â”‚   â”‚
â”‚   â”œâ”€â”€ python/                         # Training and data generation
â”‚   â”‚   â”œâ”€â”€ generate_training_data.py   # Fast parallel data generation
â”‚   â”‚   â”œâ”€â”€ train_analysis_models.py    # Train all analysis variants
â”‚   â”‚   â”œâ”€â”€ train_on_gpu.py             # GPU-accelerated training
â”‚   â”‚   â”œâ”€â”€ full_domain_training.py     # Full momentum range
â”‚   â”‚   â”œâ”€â”€ train_pinn.py               # Physics-informed NN (deprecated)
â”‚   â”‚   â””â”€â”€ compare_models.py           # Model comparison utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ condor/                         # HTCondor cluster jobs
â”‚   â”‚   â”œâ”€â”€ README.md                   # Cluster usage guide
â”‚   â”‚   â”œâ”€â”€ train_production.sub        # 12 production models
â”‚   â”‚   â”œâ”€â”€ train_analysis.sub          # 12 analysis models
â”‚   â”‚   â”œâ”€â”€ generate_data.sub           # Parallel data generation
â”‚   â”‚   â””â”€â”€ logs/                       # Job outputs
â”‚   â”‚
â”‚   â””â”€â”€ src/                            # C++ implementations
â”‚       â””â”€â”€ TrackMLPExtrapolator.cpp    # LHCb integration (prototype)
â”‚
â”œâ”€â”€ experiments/                        # ğŸ”¬ Experiment Archive
â”‚   â”œâ”€â”€ README.md                       # Experiment tracking
â”‚   â”œâ”€â”€ experiment_log.csv              # All experiments (dates, configs, results)
â”‚   â”‚
â”‚   â”œâ”€â”€ baseline/                       # Initial experiments
â”‚   â”‚   â”œâ”€â”€ v1_positive_qop/            # Single charge training
â”‚   â”‚   â””â”€â”€ v2_both_charges/            # Full charge spectrum
â”‚   â”‚
â”‚   â”œâ”€â”€ architecture/                   # Network architecture studies
â”‚   â”‚   â”œâ”€â”€ deeper_networks/            # Depth experiments
â”‚   â”‚   â”œâ”€â”€ wider_networks/             # Width experiments
â”‚   â”‚   â””â”€â”€ skip_connections/           # ResNet-style connections
â”‚   â”‚
â”‚   â”œâ”€â”€ momentum_studies/               # Momentum range experiments
â”‚   â”‚   â”œâ”€â”€ low_p_05_2gev/              # Low momentum (challenging)
â”‚   â”‚   â”œâ”€â”€ mid_p_2_10gev/              # Medium momentum
â”‚   â”‚   â””â”€â”€ high_p_10_100gev/           # High momentum (easier)
â”‚   â”‚
â”‚   â”œâ”€â”€ physics_informed/               # PINN experiments
â”‚   â”‚   â”œâ”€â”€ energy_conservation/        # Energy loss constraints
â”‚   â”‚   â””â”€â”€ lorentz_loss/               # Lorentz force penalties
â”‚   â”‚
â”‚   â”œâ”€â”€ data_augmentation/              # Data sampling strategies
â”‚   â”‚   â”œâ”€â”€ dense_grid/                 # Uniform sampling
â”‚   â”‚   â””â”€â”€ random_sampling/            # Random track generation
â”‚   â”‚
â”‚   â”œâ”€â”€ field_maps/                     # Magnetic field studies
â”‚   â”‚   â”œâ”€â”€ simplified/                 # Simplified B-field
â”‚   â”‚   â””â”€â”€ simcond/                    # Full simulation conditions
â”‚   â”‚
â”‚   â”œâ”€â”€ weighted_loss/                  # Loss function experiments
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ train_weighted.py
â”‚   â”‚   â””â”€â”€ training_log.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ onnx_export/                    # Model export (for deployment)
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ export_onnx.py
â”‚   â”‚   â”œâ”€â”€ mlp_full_domain.onnx
â”‚   â”‚   â””â”€â”€ pinn_full_domain.onnx
â”‚   â”‚
â”‚   â””â”€â”€ production/                     # Production-ready models
â”‚       â””â”€â”€ best_model/                 # Finalized model for deployment
â”‚
â”œâ”€â”€ src/                                # ğŸ”§ Traditional C++ Extrapolators (Reference)
â”‚   â”œâ”€â”€ TrackRungeKuttaExtrapolator.cpp    # Gold standard (slow but accurate)
â”‚   â”œâ”€â”€ TrackKiselExtrapolator.cpp         # Fast numerical method
â”‚   â”œâ”€â”€ TrackHerabExtrapolator.cpp         # Alternative fast method
â”‚   â”œâ”€â”€ TrackLinearExtrapolator.cpp        # Simplest approximation
â”‚   â”œâ”€â”€ TrackParabolicExtrapolator.cpp     # Second-order approximation
â”‚   â”œâ”€â”€ TrackFieldExtrapolatorBase.cpp     # Base class
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ tests/                              # Test configurations
â”‚   â”œâ”€â”€ options/
â”‚   â”œâ”€â”€ qmtest/
â”‚   â””â”€â”€ refs/
â”‚
â”œâ”€â”€ report/                             # ğŸ“„ Documentation
â”‚   â”œâ”€â”€ pinn_track_extrapolation_report.tex
â”‚   â””â”€â”€ pinn_track_extrapolation_report.pdf
â”‚
â”œâ”€â”€ plots/                              # Generated figures
â”œâ”€â”€ doc/                                # Release notes
â”œâ”€â”€ lhcb-metainfo/                      # LHCb metadata
â””â”€â”€ CMakeLists.txt                      # Build configuration
```

---

## ğŸš€ Quick Start

### 1. Explore Results (Recommended First Step)

Open the main analysis notebook:
```bash
cd /data/bfys/gscriven/TE_stack/Rec/Tr/TrackExtrapolators
jupyter notebook model_investigation.ipynb
```

**What you'll find:**
- âœ… Model performance comparison (activation functions, PINN analysis)
- âœ… Physics constraint analysis using autograd
- âœ… Feature sensitivity and gradient analysis
- âœ… Why physics-informed models failed (detailed diagnosis)
- âœ… Decision boundaries and non-linearity visualization

### 2. Train Models Locally

```bash
cd ml_models/python

# Generate training data (fast parallel version)
python generate_training_data.py --samples 50000 --output ../data/ --name analysis

# Train analysis models (all variants)
python train_analysis_models.py

# Or train single model on GPU
python train_on_gpu.py --hidden 256 256 128 64 --epochs 2000 --name custom_model
```

### 3. Submit Cluster Jobs (HTCondor)

```bash
cd ml_models/condor

# Generate large dataset (parallel)
condor_submit generate_data.sub

# Train production models (12 architectures)
condor_submit train_production.sub

# Monitor jobs
condor_q

# Check outputs
tail -f logs/prod_*.out
```

See [`ml_models/condor/README.md`](ml_models/condor/README.md) for detailed cluster usage.

---

## ğŸ“Š Key Results Summary

### Current Best Model: **MLP with SiLU Activation**

| Metric | Value |
|--------|-------|
| **Mean Error** | 0.21 mm |
| **Median Error** | 0.15 mm |
| **P95 Error** | 0.54 mm |
| **Max Error** | 4.13 mm |
| **Architecture** | 128-128-64 (3 hidden layers) |
| **Parameters** | 25,924 |
| **Activation** | SiLU (Swish) |

### Activation Function Comparison

| Activation | Mean Error | P95 Error | Speedup vs Tanh |
|------------|------------|-----------|-----------------|
| **SiLU** â­ | 0.21 mm | 0.54 mm | - |
| Tanh | 0.63 mm | 1.68 mm | 3Ã— worse |
| ReLU | 0.77 mm | 1.78 mm | 3.7Ã— worse |

### PINN Failure Analysis

All physics-informed models **failed dramatically**:

| PINN Î» | Mean Error | Why It Failed |
|--------|------------|---------------|
| 0.01 | 18.8 mm | Wrong physics formulation |
| 0.05 | 106.7 mm | Conflicting constraints |
| 0.1 | 197.2 mm | No proper Lorentz integration |
| 0.2 | 328.9 mm | Higher Î» â†’ worse (contradicts true physics!) |

**Root cause:** Physics loss uses oversimplified straight-line approximation instead of proper magnetic field ODE integration. See detailed analysis in [`model_investigation.ipynb`](model_investigation.ipynb).

---

## ğŸ§  Network Architecture

### Input (6D) @ z = 4000 mm
- `x, y` - Position [mm]
- `tx, ty` - Slopes dx/dz, dy/dz [dimensionless]
- `q/p` - Signed inverse momentum [GeVâ»Â¹]
- `z` - Longitudinal position [mm] (currently fixed)

### Output (4D) @ z = 12000 mm
- `x', y'` - Extrapolated position [mm]
- `tx', ty'` - Extrapolated slopes

**What the network learns:** The magnetic field propagator over Î”z = 8000 mm, including non-uniform B(x,y,z), Lorentz force bending, and geometric effectsâ€”all implicitly from data!

---

## ğŸ“ˆ Next Steps

### Immediate (Production Ready)
1. âœ… **Complete** - Identify best architecture (SiLU activation)
2. â³ **In Progress** - Train production models on full dataset (HTCondor)
3. ğŸ”œ **Next** - Export to ONNX for C++ inference
4. ğŸ”œ **Next** - Integrate into LHCb framework
5. ğŸ”œ **Next** - Benchmark against Runge-Kutta on real events

### Medium Term (Improved Physics)
- Redesign PINN with proper Lorentz force ODE integration
- Incorporate actual LHCb B(x,y,z) field map
- Add material multiple scattering effects
- Physics-inspired feature engineering (momentum-dependent weights)

### Long Term (Advanced Methods)
- Transformer architecture for multi-step extrapolation
- Neural ODEs for continuous-time modeling
- Uncertainty quantification (Bayesian NNs)
- Active learning for data efficiency

---

## ğŸ“ Key Files

| File | Purpose |
|------|---------|
| [`model_investigation.ipynb`](model_investigation.ipynb) | **Main analysis** - comprehensive model study |
| [`ml_models/README.md`](ml_models/README.md) | ML pipeline documentation |
| [`ml_models/condor/README.md`](ml_models/condor/README.md) | Cluster job submission guide |
| [`experiments/README.md`](experiments/README.md) | Experiment tracking and history |
| `extrapolator_results.csv` | Benchmark comparison (ML vs traditional) |

---

## ğŸ”§ Technical Details

### Data Generation
- **Source:** LHCb Gaudi framework (TrackRungeKuttaExtrapolator)
- **Sampling:** Random uniform in phase space
- **Coverage:** 0.5-100 GeV/c momentum, Â±1000 mm position, Â±0.3 slope
- **Size:** 50K samples for analysis, scaling to 1M+ for production

### Training
- **Framework:** PyTorch 2.9.1 + CUDA
- **Optimizer:** AdamW with weight decay
- **Scheduler:** ReduceLROnPlateau
- **Loss:** MSE on position + slopes (+ physics loss for PINN)
- **Hardware:** NVIDIA L40S GPUs (45GB VRAM, Capability 8.9)
- **Time:** ~2-5 min per model on GPU

### Evaluation
- **Metric:** Position error = âˆš((x_pred - x_true)Â² + (y_pred - y_true)Â²)
- **Validation:** 20% held-out test set
- **Physics checks:** Slope ratios, bending consistency, momentum dependence

---

## ğŸ‘¥ Contributors

- George Scriven (gscriven@nikhef.nl)
- LHCb Reconstruction Group

---

## ğŸ“š References

- LHCb Track Reconstruction: [LHCb-2007-007](https://cds.cern.ch/record/1033584)
- Physics-Informed Neural Networks: [Raissi et al. 2019](https://www.sciencedirect.com/science/article/pii/S0021999118307125)
- Neural ODEs: [Chen et al. 2018](https://arxiv.org/abs/1806.07366)

---

## ğŸ“„ License

LHCb Software - see LHCb collaboration policies

---

**Last Updated:** January 2025  
**Version:** 1.0 (Analysis Phase Complete)
